{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed3bc791-c449-4dfe-a111-9d82587c081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from collections import Counter\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from groq import Groq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "814586b2-09b7-4df1-b774-71ed93c1258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"./cache/\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\"\n",
    "os.environ['OPENAI_API_KEY'] = 'openai-api-key-here'\n",
    "os.environ['ANTHROPIC_API_KEY'] = 'anthropic-api-key-here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fd63e41c-2575-4e73-bd79-9511b289d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from tqdm import tqdm\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define the schema for sentiment analysis\n",
    "# Define the schema for sentiment analysis\n",
    "class SentimentAnalysisResponse(BaseModel):\n",
    "    sentiment: str = Field(description=\"The sentiment of the sentence (Positive, Negative, or Neutral)\")\n",
    "\n",
    "class Data(BaseModel):\n",
    "    \"\"\"Extracted data about sentences.\"\"\"\n",
    "    sentiments: List[SentimentAnalysisResponse]\n",
    "\n",
    "# Define the prompt template\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Your task is to analyze the provided sentences written in African American English and identify the sentiment expressed by the author. \n",
    "            The sentiment should be classified as Positive, Negative, or Neutral only. No other words are allowed for the sentiment.\"\"\"\n",
    "        ),\n",
    "        (\"user\", \"{sentences}\")\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ce5c34-8b65-4995-9d97-344dfe2a1641",
   "metadata": {},
   "source": [
    "# GPT 3-5 SAE_FROM_AAE sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9eeed99d-577a-4b65-a99f-c6e47261e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the language model\n",
    "#model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "\n",
    "model = ChatAnthropic(model=\"claude-3-haiku-20240307\", timeout=None,\n",
    "    max_retries=2, temperature=0)\n",
    "\n",
    "# Create the runnable chain\n",
    "runnable = chat_template | model.with_structured_output(schema=Data)\n",
    "runnable_single = chat_template | model.with_structured_output(schema=SentimentAnalysisResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "19fad9e3-c27c-4656-a613-a301531c4658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentAnalysisResponse(sentiment='Neutral')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable_single.invoke({\"sentences\" : \"I keep falling in and out of love with you. Sometimes I love you, sometimes you make me sad.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87070139-487f-438e-9668-bb23250100e2",
   "metadata": {},
   "source": [
    "## Read the AAE from SAE dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f2e2b11-bd3c-498c-a188-3422de428037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>standard_american_english</th>\n",
       "      <th>aae_from_sae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm not giving that woman anything but bubble ...</td>\n",
       "      <td>Nah, I ain't givin' that woman nothin' but som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes, that's what's up. There's nothing like ge...</td>\n",
       "      <td>Yeah, that's what's up. Ain't nothin' like get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Mixed, huh? Those dark elbows and knees will g...</td>\n",
       "      <td>Mixed, huh? Them dark elbows and knees gon' gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The player Mike James from the Dallas Maverick...</td>\n",
       "      <td>That player Mike James from the Dallas Maveric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>It took a complete stranger to tell me they're...</td>\n",
       "      <td>It took a complete stranger to tell me they pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                          standard_american_english  \\\n",
       "0      0  I'm not giving that woman anything but bubble ...   \n",
       "1      1  Yes, that's what's up. There's nothing like ge...   \n",
       "2      2  Mixed, huh? Those dark elbows and knees will g...   \n",
       "3      3  The player Mike James from the Dallas Maverick...   \n",
       "4      4  It took a complete stranger to tell me they're...   \n",
       "\n",
       "                                        aae_from_sae  \n",
       "0  Nah, I ain't givin' that woman nothin' but som...  \n",
       "1  Yeah, that's what's up. Ain't nothin' like get...  \n",
       "2  Mixed, huh? Them dark elbows and knees gon' gi...  \n",
       "3  That player Mike James from the Dallas Maveric...  \n",
       "4  It took a complete stranger to tell me they pr...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"./labeled/anthropic_Haiku-AAE_from_SAE_updated.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce4cf7ce-3a8a-4130-bd13-25c6b3892990",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['aae_from_sae']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "192190a1-9495-4079-9aae-06112320cd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 400/400 [06:54<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled dataset saved to ./labeled/anthropic_Haiku-AAE_from_SAE_updated_sentiment.csv\n",
      "Failed indices saved to ./labeled/failed_indices.csv\n",
      "Number of successfully processed sentences: 1970\n",
      "Number of failed sentences: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store the sentiments and their corresponding indices\n",
    "all_sentiments = []\n",
    "processed_indices = []\n",
    "failed_indices = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(dataset), 5)):\n",
    "    batch = dataset[i:i+5].to_list()\n",
    "    batch_indices = list(range(i, min(i+5, len(dataset))))\n",
    "    \n",
    "    try:\n",
    "        result = runnable.invoke({\"sentences\": \"\\n\".join(batch)})\n",
    "        \n",
    "        # Check if the number of returned sentiments matches the batch size\n",
    "        if len(result.sentiments) == len(batch):\n",
    "            all_sentiments.extend([response.sentiment for response in result.sentiments])\n",
    "            processed_indices.extend(batch_indices)\n",
    "        else:\n",
    "            # If the number of sentiments doesn't match, mark all as failed\n",
    "            failed_indices.extend(batch_indices)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {i}-{i+4}: {str(e)}\")\n",
    "        failed_indices.extend(batch_indices)\n",
    "\n",
    "# Create a new dataframe with successfully processed sentences and sentiments\n",
    "labeled_df = pd.DataFrame({\n",
    "    'index': processed_indices,\n",
    "    'aae_from_sae': dataset.iloc[processed_indices],\n",
    "    'sentiment': all_sentiments\n",
    "})\n",
    "\n",
    "# Sort the dataframe by the original index\n",
    "labeled_df = labeled_df.sort_values('index').reset_index(drop=True)\n",
    "\n",
    "# Save the labeled dataset to a CSV file\n",
    "output_path = './labeled/anthropic_Haiku-AAE_from_SAE_updated_sentiment.csv'\n",
    "labeled_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Labeled dataset saved to {output_path}\")\n",
    "\n",
    "# Save the failed indices to a separate file\n",
    "failed_indices_path = './labeled/failed_indices.csv'\n",
    "pd.DataFrame({'failed_index': failed_indices}).to_csv(failed_indices_path, index=False)\n",
    "\n",
    "print(f\"Failed indices saved to {failed_indices_path}\")\n",
    "print(f\"Number of successfully processed sentences: {len(processed_indices)}\")\n",
    "print(f\"Number of failed sentences: {len(failed_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c4bd3b-4d80-4610-b6dc-bc8d4be58f87",
   "metadata": {},
   "source": [
    "## Handling failed indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4f296d4-5541-49d0-b89d-f83923466c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 15/15 [00:15<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete labeled dataset saved to ./labeled/complete-2000-anthropic_Haiku-AAE_from_SAE_updated.csv\n",
      "Total processed sentences: 2000\n",
      "Successfully labeled sentences: 1992\n",
      "Failed sentences: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the previously processed data\n",
    "labeled_df = pd.read_csv('./labeled/anthropic_Haiku-AAE_from_SAE_updated_sentiment.csv')\n",
    "failed_indices = pd.read_csv('./labeled/failed_indices.csv')['failed_index'].tolist()\n",
    "\n",
    "# Load the original dataset\n",
    "original_dataset = pd.read_csv(\"./labeled/anthropic_Haiku-AAE_from_SAE_updated.csv\")['aae_from_sae']\n",
    "\n",
    "# Initialize lists to store the new sentiments and their corresponding indices\n",
    "new_sentiments = []\n",
    "new_processed_indices = []\n",
    "still_failed_indices = []\n",
    "\n",
    "# Process the failed sentences\n",
    "for i in tqdm(range(0, len(failed_indices), 2)):\n",
    "    batch_indices = failed_indices[i:i+2]\n",
    "    batch = original_dataset.iloc[batch_indices].tolist()\n",
    "    \n",
    "    try:\n",
    "        result = runnable.invoke({\"sentences\": \"\\n\".join(batch)})\n",
    "        \n",
    "        # Check if the number of returned sentiments matches the batch size\n",
    "        if len(result.sentiments) == len(batch):\n",
    "            new_sentiments.extend([response.sentiment for response in result.sentiments])\n",
    "            new_processed_indices.extend(batch_indices)\n",
    "        else:\n",
    "            # If the number of sentiments doesn't match, mark all as still failed\n",
    "            still_failed_indices.extend(batch_indices)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing index {i}-{i+1}: {str(e)}\")\n",
    "        still_failed_indices.extend(batch_indices)\n",
    "\n",
    "# Create a new dataframe with newly processed sentences and sentiments\n",
    "new_labeled_df = pd.DataFrame({\n",
    "    'index': new_processed_indices,\n",
    "    'aae_from_sae': original_dataset.iloc[new_processed_indices],\n",
    "    'sentiment': new_sentiments\n",
    "})\n",
    "\n",
    "# Combine the previously processed data with the newly processed data\n",
    "combined_df = pd.concat([labeled_df, new_labeled_df], ignore_index=True)\n",
    "\n",
    "# Sort the dataframe by the original index and reset the index\n",
    "combined_df = combined_df.sort_values('index').reset_index(drop=True)\n",
    "\n",
    "# If there are still failed indices, add them to the combined dataframe with NaN sentiment\n",
    "if still_failed_indices:\n",
    "    failed_df = pd.DataFrame({\n",
    "        'index': still_failed_indices,\n",
    "        'aae_from_sae': original_dataset.iloc[still_failed_indices],\n",
    "        'sentiment': pd.NA\n",
    "    })\n",
    "    combined_df = pd.concat([combined_df, failed_df], ignore_index=True)\n",
    "    combined_df = combined_df.sort_values('index').reset_index(drop=True)\n",
    "\n",
    "# Save the complete labeled dataset to a CSV file\n",
    "output_path = './labeled/complete-2000-anthropic_Haiku-AAE_from_SAE_updated.csv'\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Complete labeled dataset saved to {output_path}\")\n",
    "print(f\"Total processed sentences: {len(combined_df)}\")\n",
    "print(f\"Successfully labeled sentences: {combined_df['sentiment'].notna().sum()}\")\n",
    "print(f\"Failed sentences: {combined_df['sentiment'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3473ce56-8144-4ed2-99b1-43174f6825ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.read_csv('./labeled/complete-2000-anthropic_Haiku-AAE_from_SAE_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15472498-529b-4426-9fee-2afa6fbb957c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Negative' 'Positive' 'Neutral' 'Mixed' '<UNKNOWN>']\n"
     ]
    }
   ],
   "source": [
    "# Function to replace null sentiments\n",
    "def replace_null_sentiments(df, api_function):\n",
    "    # Identify rows where sentiment is null\n",
    "    null_sentiments = df[df['sentiment'].isna()]\n",
    "    \n",
    "    # Loop through these rows\n",
    "    for index, row in null_sentiments.iterrows():\n",
    "        text = row['aae_from_sae']\n",
    "        # Call the API or function to get the sentiment\n",
    "        result = api_function.invoke({\"sentences\": text}).sentiment\n",
    "        # Update the DataFrame with the returned sentiment\n",
    "        df.at[index, 'sentiment'] = result\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Call the function\n",
    "new_combined_df = replace_null_sentiments(combined_df, runnable_single)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(new_combined_df['sentiment'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "36ac5009-fe35-4325-9d70-09ae490ada6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>aae_from_sae</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>1434</td>\n",
       "      <td>Nah, I ain't got no link fo' dat info. I'm 'bo...</td>\n",
       "      <td>&lt;UNKNOWN&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                       aae_from_sae  sentiment\n",
       "1434   1434  Nah, I ain't got no link fo' dat info. I'm 'bo...  <UNKNOWN>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_combined_df[new_combined_df['sentiment'] == '<UNKNOWN>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7612b171-6d32-4903-b008-276dd6520327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Negative' 'Positive' 'Neutral']\n"
     ]
    }
   ],
   "source": [
    "print(new_combined_df['sentiment'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d1ee8767-fb3b-4912-8697-b45c9b5dbdae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable_single.invoke({\"sentences\": new_combined_df.iloc[1434].aae_from_sae}).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5dd1bffe-7936-4a3e-9369-eaab83750201",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_combined_df.at[1434, 'sentiment'] = runnable_single.invoke({\"sentences\": new_combined_df.iloc[1434].aae_from_sae}).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9f0aaff1-56df-47c6-9f3d-c066401eb3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processed sentences: 2000\n",
      "Successfully labeled sentences: 2000\n",
      "Failed sentences: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total processed sentences: {len(new_combined_df)}\")\n",
    "print(f\"Successfully labeled sentences: {new_combined_df['sentiment'].notna().sum()}\")\n",
    "print(f\"Failed sentences: {new_combined_df['sentiment'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d86b7150-e277-4347-9898-dd19d944d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_combined_df.to_csv('./labeled/complete-2000-anthropic_Haiku-AAE_from_SAE_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82c8595-320b-4815-8ec0-5b2c01de9e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "augment",
   "language": "python",
   "name": "augmentation-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
