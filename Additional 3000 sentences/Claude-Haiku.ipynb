{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d103a174-ac76-4f10-a338-706601a9b112",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from collections import Counter\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from groq import Groq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_anthropic import ChatAnthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54aa2db1-6f12-4089-921e-4488b53f5a27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cache_dir = \"../cache/\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\"\n",
    "os.environ['ANTHROPIC_API_KEY'] = 'anthropic-api-key-here'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0598cf-a9c9-4099-b3a0-45d7b423c79f",
   "metadata": {},
   "source": [
    "## The below is API script to get labels from Claude Haiku\n",
    "\n",
    "We send multiple sentences at once to save token usage cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b9b33da-3815-446b-a1c0-928537bf9892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 sentences sentiment result: sentiments=[SentimentAnalysisResponse(sentiment='Negative'), SentimentAnalysisResponse(sentiment='Positive'), SentimentAnalysisResponse(sentiment='Positive'), SentimentAnalysisResponse(sentiment='Neutral'), SentimentAnalysisResponse(sentiment='Positive'), SentimentAnalysisResponse(sentiment='Neutral'), SentimentAnalysisResponse(sentiment='Neutral'), SentimentAnalysisResponse(sentiment='Negative'), SentimentAnalysisResponse(sentiment='Neutral'), SentimentAnalysisResponse(sentiment='Neutral')]\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "from tqdm import tqdm\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define the schema for sentiment analysis\n",
    "class SentimentAnalysisResponse(BaseModel):\n",
    "    sentiment: str = Field(description=\"The sentiment of the sentence (Positive, Negative, or Neutral)\")\n",
    "\n",
    "class Data(BaseModel):\n",
    "    \"\"\"Extracted data about sentences.\"\"\"\n",
    "    sentiments: List[SentimentAnalysisResponse]\n",
    "\n",
    "# Define the prompt template\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Your task is to analyze the provided sentences written in African American English and identify the sentiment expressed by the author. \n",
    "            The sentiment should be classified as Positive, Negative, or Neutral for each sentence.\"\"\"\n",
    "        ),\n",
    "        (\"user\", \"{sentences}\")\n",
    "    ]\n",
    ")\n",
    "model = ChatAnthropic(model=\"claude-3-haiku-20240307\", timeout=None,\n",
    "    max_retries=2, temperature=0)\n",
    "\n",
    "# Create the runnable chain\n",
    "runnable = chat_template | model.with_structured_output(schema=Data)\n",
    "runnable_single = chat_template | model.with_structured_output(schema=SentimentAnalysisResponse)\n",
    "\n",
    "dataset = pd.read_csv('2000-5000_sentences.csv')[\"text\"]\n",
    "\n",
    "## Testing on first 10 sentences\n",
    "result = runnable.invoke({\"sentences\" : \"\\n\".join(dataset[:10].to_list())})\n",
    "\n",
    "print(f\"First 10 sentences sentiment result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0b08167-4189-4d59-a8e0-379ecad51801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|███████████████████████████▏                                                                                                                                                                                        | 77/600 [01:26<08:39,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing batch 380-384: 1 validation error for Data\n",
      "sentiments\n",
      "  value is not a valid list (type=type_error.list)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|████████████████████████████████████████████████▌                                                                                                                                                                  | 138/600 [02:34<07:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing batch 685-689: 1 validation error for Data\n",
      "sentiments\n",
      "  value is not a valid list (type=type_error.list)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 600/600 [10:01<00:00,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled dataset saved to ./labeled/Claude-Haiku-Labels.csv\n",
      "Failed indices saved to ./labeled/failed_indices=Claude-Haiku.csv\n",
      "Number of successfully processed sentences: 2750\n",
      "Number of failed sentences: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store the sentiments and their corresponding indices\n",
    "all_sentiments = []\n",
    "processed_indices = []\n",
    "failed_indices = []\n",
    "\n",
    "# Process the dataset in batches of 5\n",
    "for i in tqdm(range(0, len(dataset), 5)):\n",
    "    batch = dataset[i:i+5].to_list()\n",
    "    batch_indices = list(range(i, min(i+5, len(dataset))))\n",
    "    \n",
    "    try:\n",
    "        result = runnable.invoke({\"sentences\": \"\\n\".join(batch)})\n",
    "        \n",
    "        # Check if the number of returned sentiments matches the batch size\n",
    "        if len(result.sentiments) == len(batch):\n",
    "            all_sentiments.extend([response.sentiment for response in result.sentiments])\n",
    "            processed_indices.extend(batch_indices)\n",
    "        else:\n",
    "            # If the number of sentiments doesn't match, mark all as failed\n",
    "            failed_indices.extend(batch_indices)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {i}-{i+4}: {str(e)}\")\n",
    "        failed_indices.extend(batch_indices)\n",
    "\n",
    "# Create a new dataframe with successfully processed sentences and sentiments\n",
    "labeled_df = pd.DataFrame({\n",
    "    'index': processed_indices,\n",
    "    'text': dataset.iloc[processed_indices],\n",
    "    'sentiment': all_sentiments\n",
    "})\n",
    "\n",
    "# Sort the dataframe by the original index\n",
    "labeled_df = labeled_df.sort_values('index').reset_index(drop=True)\n",
    "\n",
    "# Save the labeled dataset to a CSV file\n",
    "output_path = './labeled/Claude-Haiku-Labels.csv'\n",
    "labeled_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Labeled dataset saved to {output_path}\")\n",
    "\n",
    "# Save the failed indices to a separate file\n",
    "failed_indices_path = './labeled/failed_indices=Claude-Haiku.csv'\n",
    "pd.DataFrame({'failed_index': failed_indices}).to_csv(failed_indices_path, index=False)\n",
    "\n",
    "print(f\"Failed indices saved to {failed_indices_path}\")\n",
    "print(f\"Number of successfully processed sentences: {len(processed_indices)}\")\n",
    "print(f\"Number of failed sentences: {len(failed_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4608966b-78f1-44cf-b56e-45440cb2a635",
   "metadata": {},
   "source": [
    "## Processing indcies that were not parsed correctly in first iteration of the API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92156b02-5035-4d92-917f-499229988d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 84/84 [01:16<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete labeled dataset saved to ./labeled/complete-2000-5000-Claude-Haiku-Labels.csv\n",
      "Total processed sentences: 3000\n",
      "Successfully labeled sentences: 2990\n",
      "Failed sentences: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the previously processed data\n",
    "labeled_df = pd.read_csv('./labeled/Claude-Haiku-Labels.csv')\n",
    "failed_indices = pd.read_csv('./labeled/failed_indices=Claude-Haiku.csv')['failed_index'].tolist()\n",
    "\n",
    "# Load the original dataset\n",
    "original_dataset = pd.read_csv('2000-5000_sentences.csv')[\"text\"]\n",
    "\n",
    "# Initialize lists to store the new sentiments and their corresponding indices\n",
    "new_sentiments = []\n",
    "new_processed_indices = []\n",
    "still_failed_indices = []\n",
    "\n",
    "# Process the failed sentences\n",
    "for i in tqdm(range(0, len(failed_indices), 3)):\n",
    "    batch_indices = failed_indices[i:i+3]\n",
    "    batch = original_dataset.iloc[batch_indices].tolist()\n",
    "    \n",
    "    try:\n",
    "        result = runnable.invoke({\"sentences\": \"\\n\".join(batch)})\n",
    "        \n",
    "        # Check if the number of returned sentiments matches the batch size\n",
    "        if len(result.sentiments) == len(batch):\n",
    "            new_sentiments.extend([response.sentiment for response in result.sentiments])\n",
    "            new_processed_indices.extend(batch_indices)\n",
    "        else:\n",
    "            # If the number of sentiments doesn't match, mark all as still failed\n",
    "            still_failed_indices.extend(batch_indices)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {i}-{i+2}: {str(e)}\")\n",
    "        still_failed_indices.extend(batch_indices)\n",
    "\n",
    "# Create a new dataframe with newly processed sentences and sentiments\n",
    "new_labeled_df = pd.DataFrame({\n",
    "    'index': new_processed_indices,\n",
    "    'text': original_dataset.iloc[new_processed_indices],\n",
    "    'sentiment': new_sentiments\n",
    "})\n",
    "\n",
    "# Combine the previously processed data with the newly processed data\n",
    "combined_df = pd.concat([labeled_df, new_labeled_df], ignore_index=True)\n",
    "\n",
    "# Sort the dataframe by the original index and reset the index\n",
    "combined_df = combined_df.sort_values('index').reset_index(drop=True)\n",
    "\n",
    "# If there are still failed indices, add them to the combined dataframe with NaN sentiment\n",
    "if still_failed_indices:\n",
    "    failed_df = pd.DataFrame({\n",
    "        'index': still_failed_indices,\n",
    "        'text': original_dataset.iloc[still_failed_indices],\n",
    "        'sentiment': pd.NA\n",
    "    })\n",
    "    combined_df = pd.concat([combined_df, failed_df], ignore_index=True)\n",
    "    combined_df = combined_df.sort_values('index').reset_index(drop=True)\n",
    "\n",
    "# Save the complete labeled dataset to a CSV file\n",
    "output_path = './labeled/complete-2000-5000-Claude-Haiku-Labels.csv'\n",
    "combined_df[[\"text\", \"sentiment\"]].to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Complete labeled dataset saved to {output_path}\")\n",
    "print(f\"Total processed sentences: {len(combined_df)}\")\n",
    "print(f\"Successfully labeled sentences: {combined_df['sentiment'].notna().sum()}\")\n",
    "print(f\"Failed sentences: {combined_df['sentiment'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7619f670-09b6-42ab-91ed-a9013d51142f",
   "metadata": {},
   "source": [
    "### Individualy calling the API for the 10 falied indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4de55e8b-4639-4064-967b-71996231f431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:10<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated complete labeled dataset saved to ./labeled/ccomplete-2000-5000-Claude-Haiku-Labels-final.csv\n",
      "Total processed sentences: 3000\n",
      "Successfully labeled sentences: 3000\n",
      "Failed sentences: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the complete labeled dataset\n",
    "combined_df = pd.read_csv('./labeled/complete-2000-5000-Claude-Haiku-Labels.csv')\n",
    "\n",
    "for idx in tqdm(combined_df[combined_df['sentiment'].isna()].index, desc=\"Processing sentences\"):\n",
    "    row = combined_df.loc[idx]\n",
    "    try:\n",
    "        # Invoke the API and update the sentiment\n",
    "        combined_df.at[idx, 'sentiment'] = runnable_single.invoke({\"sentences\": row['text']}).sentiment\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sentence at index {idx}: {str(e)}\")\n",
    "\n",
    "# Save the updated complete labeled dataset\n",
    "output_path = './labeled/ccomplete-2000-5000-Claude-Haiku-Labels-final.csv'\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Display the summary of the updated dataset\n",
    "updated_total = len(combined_df)\n",
    "updated_successful = combined_df['sentiment'].notna().sum()\n",
    "updated_failed = combined_df['sentiment'].isna().sum()\n",
    "\n",
    "print(f\"Updated complete labeled dataset saved to {output_path}\")\n",
    "print(f\"Total processed sentences: {updated_total}\")\n",
    "print(f\"Successfully labeled sentences: {updated_successful}\")\n",
    "print(f\"Failed sentences: {updated_failed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df79542d-bd4f-4a3c-9378-6c4c61218705",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[[\"text\", \"sentiment\"]].to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36897490-48de-46b8-a222-b2daf6972bc9",
   "metadata": {},
   "source": [
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f01e5b-9914-4ce5-bdd5-a0e47f10b4d2",
   "metadata": {},
   "source": [
    "## Now we will translate the AAE sentences to SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3bb4a895-270b-48e4-a2e8-bac585ee679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardEnglish(BaseModel):\n",
    "    standard_english: str = Field(description=\"The tweet converted into Standard American English.\")\n",
    "\n",
    "class Data(BaseModel):\n",
    "    \"\"\"Convert the list of tweets provided to standard american english.\"\"\"\n",
    "    standard_english_tweets: List[StandardEnglish] = Field(description=\"The list of converted tweets by order of the sentences given.\")\n",
    "\n",
    "# Define the prompt template\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You will be given a list of tweets extracted from twitter accounts belonging to African American individuals. Your task is to convert the given tweet to Standard American English.\"\"\"\n",
    "        ),\n",
    "        (\"user\", \"{sentences}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatAnthropic(model=\"claude-3-haiku-20240307\", timeout=None,\n",
    "    max_retries=2, temperature=0)\n",
    "\n",
    "# Create the runnable chain\n",
    "runnable = chat_template | model.with_structured_output(schema=Data)\n",
    "\n",
    "chat_template_single = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You will be given a tweet extracted from a twitter account belonging to an African American individual. Your task is to convert the given tweet to Standard American English.\"\"\"\n",
    "        ),\n",
    "        (\"user\", \"{sentence}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "runnable_single = chat_template_single | model.with_structured_output(schema=StandardEnglish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "633c2f94-b64a-4871-b28c-d540ff0c095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aae_dataset = pd.read_csv('2000-5000_sentences.csv')[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "158d2f32-8d37-4f1a-87b8-1bfeaaedc04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sae_sentence = []\n",
    "processed_indices = []\n",
    "failed_indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d036ce68-7615-45cc-8275-f55701c0dd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 600/600 [15:43<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled dataset saved to ./labeled/Claude-Haiku-SAE.csv\n",
      "Failed indices saved to ./labeled/failed_indices_Claude-Haiku-SAE.csv\n",
      "Number of successfully processed sentences: 2470\n",
      "Number of failed sentences: 530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process the dataset in batches of 5\n",
    "for i in tqdm(range(0, len(aae_dataset), 5)):\n",
    "    batch = aae_dataset[i:i+5].to_list()\n",
    "    batch_indices = list(range(i, min(i+5, len(aae_dataset))))\n",
    "    \n",
    "    try:\n",
    "        result = runnable.invoke({\"sentences\": \"\\n\".join(batch)})\n",
    "        \n",
    "        # Check if the number of returned sentiments matches the batch size\n",
    "        if len(result.standard_english_tweets) == len(batch):\n",
    "            all_sae_sentence.extend([response.standard_english for response in result.standard_english_tweets])\n",
    "            processed_indices.extend(batch_indices)\n",
    "        else:\n",
    "            # If the number of sentiments doesn't match, mark all as failed\n",
    "            failed_indices.extend(batch_indices)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {i}-{i+4}: {str(e)}\")\n",
    "        failed_indices.extend(batch_indices)\n",
    "\n",
    "# Create a new dataframe with successfully processed sentences and sentiments\n",
    "labeled_df = pd.DataFrame({\n",
    "    'index': processed_indices,\n",
    "    'african_american_english': aae_dataset.iloc[processed_indices],\n",
    "    'standard_american_english': all_sae_sentence\n",
    "})\n",
    "\n",
    "# Sort the dataframe by the original index\n",
    "labeled_df = labeled_df.sort_values('index').reset_index(drop=True)\n",
    "\n",
    "# Save the labeled dataset to a CSV file\n",
    "output_path = './labeled/Claude-Haiku-SAE.csv'\n",
    "labeled_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Labeled dataset saved to {output_path}\")\n",
    "\n",
    "# Save the failed indices to a separate file\n",
    "failed_indices_path = './labeled/failed_indices_Claude-Haiku-SAE.csv'\n",
    "pd.DataFrame({'failed_index': failed_indices}).to_csv(failed_indices_path, index=False)\n",
    "\n",
    "print(f\"Failed indices saved to {failed_indices_path}\")\n",
    "print(f\"Number of successfully processed sentences: {len(processed_indices)}\")\n",
    "print(f\"Number of failed sentences: {len(failed_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07465af9-f63b-4da4-b06e-dacc2344566a",
   "metadata": {},
   "source": [
    "## Processing the failed 530 sentences with lower batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "019acdf8-c7a6-4f94-8f33-b2a722d95047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 177/177 [03:31<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete labeled dataset saved to ./labeled/complete-3000-Claude-Haiku-SAE.csv\n",
      "Total processed sentences: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the previously processed data\n",
    "labeled_df = pd.read_csv('./labeled/Claude-Haiku-SAE.csv')\n",
    "failed_indices = pd.read_csv('./labeled/failed_indices_Claude-Haiku-SAE.csv')['failed_index'].tolist()\n",
    "\n",
    "new_all_sae_sentence = []\n",
    "new_processed_indices = []\n",
    "still_failed_indices = []\n",
    "\n",
    "# Process the failed sentences\n",
    "for i in tqdm(range(0, len(failed_indices), 3)):\n",
    "    batch_indices = failed_indices[i:i+3]\n",
    "    batch = aae_dataset.iloc[batch_indices].tolist()\n",
    "    \n",
    "    try:\n",
    "        result = runnable.invoke({\"sentences\": \"\\n\".join(batch)})\n",
    "        \n",
    "        \n",
    "        if len(result.standard_english_tweets) == len(batch):\n",
    "            new_all_sae_sentence.extend([response.standard_english for response in result.standard_english_tweets])\n",
    "            new_processed_indices.extend(batch_indices)\n",
    "        else:\n",
    "            # If the number of sentiments doesn't match, mark all as still failed\n",
    "            still_failed_indices.extend(batch_indices)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {i}-{i+2}: {str(e)}\")\n",
    "        still_failed_indices.extend(batch_indices)\n",
    "\n",
    "# Create a new dataframe with newly processed sentences and sentiments\n",
    "new_labeled_df = pd.DataFrame({\n",
    "    'index': new_processed_indices,\n",
    "    'african_american_english': aae_dataset.iloc[new_processed_indices],\n",
    "    'standard_american_english': new_all_sae_sentence\n",
    "})\n",
    "\n",
    "# Combine the previously processed data with the newly processed data\n",
    "combined_df = pd.concat([labeled_df, new_labeled_df], ignore_index=True)\n",
    "\n",
    "# Sort the dataframe by the original index and reset the index\n",
    "combined_df = combined_df.sort_values('index').reset_index(drop=True)\n",
    "\n",
    "# If there are still failed indices, add them to the combined dataframe with NaN sentiment\n",
    "if still_failed_indices:\n",
    "    failed_df = pd.DataFrame({\n",
    "        'index': still_failed_indices,\n",
    "        'african_american_english': aae_dataset.iloc[still_failed_indices],\n",
    "        'standard_american_english': pd.NA\n",
    "    })\n",
    "    combined_df = pd.concat([combined_df, failed_df], ignore_index=True)\n",
    "    combined_df = combined_df.sort_values('index').reset_index(drop=True)\n",
    "\n",
    "output_path = './labeled/complete-3000-Claude-Haiku-SAE.csv'\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Complete labeled dataset saved to {output_path}\")\n",
    "print(f\"Total processed sentences: {len(combined_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d8dd2c5-c891-4219-8e70-177f9d7d02c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed indices: 218\n"
     ]
    }
   ],
   "source": [
    "print(f\"failed indices: {combined_df['standard_american_english'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081c6ea5-db77-45bf-82d1-9a60ea105f12",
   "metadata": {},
   "source": [
    "## Individually calling the API for the failed 218 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c293cd2e-c321-4e03-9745-992b1e2c429b",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Output blocked by content filtering policy'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/local/home/furquanh/tmp/ipykernel_1173090/1752135043.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Apply the lambda function to process the indices where sentiment is NaN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m combined_df['standard_american_english'] = combined_df.apply(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrunnable_single\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'african_american_english'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandard_english\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'standard_american_english'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'standard_american_english'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m  10032\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10033\u001b[0m         )\n\u001b[0;32m> 10034\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"apply\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10036\u001b[0m     def map(\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/tmp/ipykernel_1173090/1752135043.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Apply the lambda function to process the indices where sentiment is NaN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m combined_df['standard_american_english'] = combined_df.apply(\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrunnable_single\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'african_american_english'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandard_english\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'standard_american_english'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'standard_american_english'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2502\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2503\u001b[0m                     },\n\u001b[0;32m-> 2504\u001b[0;31m                 )\n\u001b[0m\u001b[1;32m   2505\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2506\u001b[0m                 \u001b[0mfield\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev_output_schema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__fields__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4572\u001b[0m     def transform(\n\u001b[0;32m-> 4573\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4574\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4575\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRunnableConfig\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mThis\u001b[0m \u001b[0mtable\u001b[0m \u001b[0mprovides\u001b[0m \u001b[0ma\u001b[0m \u001b[0mbrief\u001b[0m \u001b[0moverview\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmain\u001b[0m \u001b[0mdeclarative\u001b[0m \u001b[0mmethods\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0msee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreference\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mdocumentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mCreating\u001b[0m \u001b[0mcustom\u001b[0m \u001b[0mchat\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mCustom\u001b[0m \u001b[0mchat\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mimplementations\u001b[0m \u001b[0mshould\u001b[0m \u001b[0minherit\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthis\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mPlease\u001b[0m \u001b[0mreference\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtable\u001b[0m \u001b[0mbelow\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minformation\u001b[0m \u001b[0mabout\u001b[0m \u001b[0mwhich\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m         inheritable_metadata = {\n\u001b[1;32m    598\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ls_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m         }\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0mrun_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         )\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrate_limiter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0minheritable_metadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         )\n\u001b[1;32m    448\u001b[0m         (run_manager,) = await callback_manager.on_chat_model_start(\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;36m2.\u001b[0m \u001b[0mneed\u001b[0m \u001b[0mmore\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mjust\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtop\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m             \u001b[0;36m3.\u001b[0m \u001b[0mare\u001b[0m \u001b[0mbuilding\u001b[0m \u001b[0mchains\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mare\u001b[0m \u001b[0magnostic\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0munderlying\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m                 \u001b[0mtype\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpure\u001b[0m \u001b[0mtext\u001b[0m \u001b[0mcompletion\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0mvs\u001b[0m \u001b[0mchat\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/langchain_anthropic/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 )\n\u001b[1;32m    524\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/anthropic/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/anthropic/resources/messages.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m     ) -> Message | Stream[RawMessageStreamEvent]:\n\u001b[0;32m--> 899\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    900\u001b[0m             \u001b[0;34m\"/v1/messages\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m             body=maybe_transform(\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m         )\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def patch(\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 921\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    922\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m         return self._process_response(\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Output blocked by content filtering policy'}}"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the complete labeled dataset\n",
    "combined_df = pd.read_csv('./labeled/complete-3000-Claude-Haiku-SAE.csv')\n",
    "\n",
    "# Apply the lambda function to process the indices where sentiment is NaN\n",
    "combined_df['standard_american_english'] = combined_df.apply(\n",
    "    lambda row: runnable_single.invoke({\"sentence\": row['african_american_english']}).standard_english\n",
    "    if pd.isna(row['standard_american_english']) else row['standard_american_english'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save the updated complete labeled dataset\n",
    "output_path = './labeled/complete-3000-Claude-Haiku-SAE-FINAL.csv'\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Display the summary of the updated dataset\n",
    "updated_total = len(combined_df)\n",
    "updated_successful = combined_df['standard_american_english'].notna().sum()\n",
    "updated_failed = combined_df['standard_american_english'].isna().sum()\n",
    "\n",
    "print(f\"Updated complete labeled dataset saved to {output_path}\")\n",
    "print(f\"Total processed sentences: {updated_total}\")\n",
    "print(f\"Successfully labeled sentences: {updated_successful}\")\n",
    "print(f\"Failed sentences: {updated_failed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c27ea03-a461-443f-ac3e-0d9163756136",
   "metadata": {},
   "source": [
    "Claude blocking some output due to inappropriate words. Let's allow the model to mask these inapproaiate words so we may have a SAE translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a69a73-29f2-4a65-8a08-6d6a24d0b83f",
   "metadata": {},
   "source": [
    "*You will be given a tweet extracted from a twitter account belonging to an African American individual. Your task is to convert the given tweet to Standard American English. **In rare circumstances, you are allowed to mask any inappropriate word but try keeping your translation as accurate as possible.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "be916da5-135b-4795-b1eb-ece96fbd7512",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template_single = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You will be given a tweet extracted from a twitter account belonging to an African American individual. Your task is to convert the given tweet to Standard American English. In rare circumstances, you are allowed to sensor out any inappropriate word in your translation.\"\"\"\n",
    "        ),\n",
    "        (\"user\", \"{sentence}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "runnable_single = chat_template_single | model.with_structured_output(schema=StandardEnglish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b575b734-203e-4baa-be25-e4fc3161789b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated complete labeled dataset saved to ./labeled/complete-3000-Claude-Haiku-SAE-FINAL.csv\n",
      "Total processed sentences: 3000\n",
      "Successfully labeled sentences: 3000\n",
      "Failed sentences: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the complete labeled dataset\n",
    "combined_df = pd.read_csv('./labeled/complete-3000-Claude-Haiku-SAE.csv')\n",
    "\n",
    "# Apply the lambda function to process the indices where sentiment is NaN\n",
    "combined_df['standard_american_english'] = combined_df.apply(\n",
    "    lambda row: runnable_single.invoke({\"sentence\": row['african_american_english']}).standard_english\n",
    "    if pd.isna(row['standard_american_english']) else row['standard_american_english'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save the updated complete labeled dataset\n",
    "output_path = './labeled/complete-3000-Claude-Haiku-SAE-FINAL.csv'\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Display the summary of the updated dataset\n",
    "updated_total = len(combined_df)\n",
    "updated_successful = combined_df['standard_american_english'].notna().sum()\n",
    "updated_failed = combined_df['standard_american_english'].isna().sum()\n",
    "\n",
    "print(f\"Updated complete labeled dataset saved to {output_path}\")\n",
    "print(f\"Total processed sentences: {updated_total}\")\n",
    "print(f\"Successfully labeled sentences: {updated_successful}\")\n",
    "print(f\"Failed sentences: {updated_failed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678a1f87-02c9-49a1-8c31-0b5d60a1bb81",
   "metadata": {},
   "source": [
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9e7759-8535-455b-82b6-1fa4eeb721d2",
   "metadata": {},
   "source": [
    "## Getting sentiment labels for SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cb3204fe-8b94-4152-bded-44b6f76e6579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisResponse(BaseModel):\n",
    "    sentiment: str = Field(description=\"The sentiment of the sentence (Positive, Negative, or Neutral)\")\n",
    "\n",
    "class Data(BaseModel):\n",
    "    \"\"\"Extracted data about sentences.\"\"\"\n",
    "    sentiments: List[SentimentAnalysisResponse]\n",
    "\n",
    "# Define the prompt template\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Your task is to analyze the provided sentences written in Standard American English and identify the sentiment expressed by the author. The sentiment should be classified as Positive, Negative, or Neutral for each sentence.\"\"\"\n",
    "        ),\n",
    "        (\"user\", \"{sentences}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatAnthropic(model=\"claude-3-haiku-20240307\", timeout=None,\n",
    "    max_retries=2, temperature=0)\n",
    "\n",
    "# Create the runnable chain\n",
    "sae_labels_runnable = chat_template | model.with_structured_output(schema=Data)\n",
    "\n",
    "class SentimentAnalysisResponse(BaseModel):\n",
    "    sentiment: str = Field(description=\"The sentiment of the sentence (Positive, Negative, or Neutral)\")\n",
    "\n",
    "chat_template_single = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Your task is to analyze the provided sentence written in Standard American English and identify the sentiment expressed by the author. The sentiment should be classified as Positive, Negative, or Neutral.\"\"\"\n",
    "        ),\n",
    "        (\"user\", \"{sentences}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "sae_labels_runnable_single = chat_template_single | model.with_structured_output(schema=SentimentAnalysisResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "985149f0-a623-4514-b6b0-35704b1bd57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sae_sentiment = []\n",
    "processed_indices = []\n",
    "failed_indices = []\n",
    "sae_dataset = pd.read_csv('./labeled/Claude-Haiku-SAE.csv')['standard_american_english']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d4a91a2-207e-452c-b303-8aa903656bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If I do not get this job tomorrow, I do not know what I will do. I am at the end of my rope.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fa0ec22d-22cb-45cc-85ef-c60e39e6f06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 600/600 [10:14<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled dataset saved to ./labeled/Claude-Haiku-sae-labels.csv\n",
      "Failed indices saved to ./labeled/failed_indices_Claude-Haiku-sae-labels.csv\n",
      "Number of successfully processed sentences: 2940\n",
      "Number of failed sentences: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0, len(sae_dataset), 5)):\n",
    "    batch = sae_dataset[i:i+5].to_list()\n",
    "    batch_indices = list(range(i, min(i+5, len(sae_dataset))))\n",
    "    \n",
    "    try:\n",
    "        result = sae_labels_runnable.invoke({\"sentences\": \"\\n\".join(batch)})\n",
    "        \n",
    "        if len(result.sentiments) == len(batch):\n",
    "            all_sae_sentiment.extend([response.sentiment for response in result.sentiments])\n",
    "            processed_indices.extend(batch_indices)\n",
    "        else:\n",
    "            failed_indices.extend(batch_indices)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {i}-{i+4}: {str(e)}\")\n",
    "        failed_indices.extend(batch_indices)\n",
    "\n",
    "labeled_df_sae_labels = pd.DataFrame({\n",
    "    'index': processed_indices,\n",
    "    'standard_american_english': sae_dataset.iloc[processed_indices],\n",
    "    'sae_labels': all_sae_sentiment\n",
    "})\n",
    "\n",
    "# Sort the dataframe by the original index\n",
    "labeled_df_sae_labels = labeled_df_sae_labels.sort_values('index').reset_index(drop=True)\n",
    "\n",
    "output_path = './labeled/Claude-Haiku-sae-labels.csv'\n",
    "labeled_df_sae_labels.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Labeled dataset saved to {output_path}\")\n",
    "\n",
    "# Save the failed indices to a separate file\n",
    "failed_indices_path = './labeled/failed_indices_Claude-Haiku-sae-labels.csv'\n",
    "pd.DataFrame({'failed_index': failed_indices}).to_csv(failed_indices_path, index=False)\n",
    "\n",
    "print(f\"Failed indices saved to {failed_indices_path}\")\n",
    "print(f\"Number of successfully processed sentences: {len(processed_indices)}\")\n",
    "print(f\"Number of failed sentences: {len(failed_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651bc3c6-e108-46b0-9dda-dbb30e019df3",
   "metadata": {},
   "source": [
    "## Processing the failed 60 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0974d4af-f8bc-49aa-850f-5d8ed4436188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:26<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete labeled dataset saved to ./labeled/complete-3000-Claude-Haiku-sae-labels.csv\n",
      "Total processed sentences: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labeled_df = pd.read_csv('./labeled/Claude-Haiku-sae-labels.csv')\n",
    "failed_indices = pd.read_csv('./labeled/failed_indices_Claude-Haiku-sae-labels.csv')['failed_index'].tolist()\n",
    "\n",
    "\n",
    "new_all_sae_sentiment = []\n",
    "new_processed_indices = []\n",
    "still_failed_indices = []\n",
    "\n",
    "for i in tqdm(range(0, len(failed_indices), 2)):\n",
    "    batch_indices = failed_indices[i:i+2]\n",
    "    batch = sae_dataset.iloc[batch_indices].tolist()\n",
    "    try:\n",
    "        result = sae_labels_runnable.invoke({\"sentences\": \"\\n\".join(batch)})\n",
    "        \n",
    "        # Check if the number of returned sentiments matches the batch size\n",
    "        if len(result.sentiments) == len(batch):\n",
    "            new_all_sae_sentiment.extend([response.sentiment for response in result.sentiments])\n",
    "            new_processed_indices.extend(batch_indices)\n",
    "        else:\n",
    "            # If the number of sentiments doesn't match, mark all as still failed\n",
    "            still_failed_indices.extend(batch_indices)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {i}-{i+2}: {str(e)}\")\n",
    "        still_failed_indices.extend(batch_indices)\n",
    "\n",
    "# Create a new dataframe with newly processed sentences and sentiments\n",
    "new_labeled_df = pd.DataFrame({\n",
    "    'index': new_processed_indices,\n",
    "    'standard_american_english': sae_dataset.iloc[new_processed_indices],\n",
    "    'sae_labels': new_all_sae_sentiment\n",
    "})\n",
    "\n",
    "# Combine the previously processed data with the newly processed data\n",
    "combined_df = pd.concat([labeled_df, new_labeled_df], ignore_index=True)\n",
    "\n",
    "# Sort the dataframe by the original index and reset the index\n",
    "combined_df = combined_df.sort_values('index').reset_index(drop=True)\n",
    "\n",
    "# If there are still failed indices, add them to the combined dataframe with NaN sentiment\n",
    "if still_failed_indices:\n",
    "    failed_df = pd.DataFrame({\n",
    "        'index': still_failed_indices,\n",
    "        'standard_american_english': sae_dataset.iloc[still_failed_indices],\n",
    "        'sae_labels': pd.NA\n",
    "    })\n",
    "    combined_df = pd.concat([combined_df, failed_df], ignore_index=True)\n",
    "    combined_df = combined_df.sort_values('index').reset_index(drop=True)\n",
    "\n",
    "output_path = './labeled/complete-3000-Claude-Haiku-sae-labels.csv'\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Complete labeled dataset saved to {output_path}\")\n",
    "print(f\"Total processed sentences: {len(combined_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4f7693-06a0-4fa2-88c2-d22f8094477c",
   "metadata": {},
   "source": [
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c323bd-8e07-4152-9f02-8660c8cb4b57",
   "metadata": {},
   "source": [
    "### Pipiline uptil now:\n",
    "AAE -> AAE Sentiment -> Translate to SAE -> SAE Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ccfe5e-435d-4dd2-8ff6-aa063c1798b3",
   "metadata": {},
   "source": [
    "## Now we go back to AAE using SAE, and finish off by obtaining sentiment on that.\n",
    "\n",
    "AAE -> AAE Sentiment -> Translate to SAE -> SAE Sentiment -> **AAE_from_SAE -> AAE_from_SAE Sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "38232643-9e15-4349-990d-67eb351861b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardEnglish(BaseModel):\n",
    "    african_american_english: str = Field(description=\"The tweet converted into African American English.\")\n",
    "\n",
    "class Data(BaseModel):\n",
    "    \"\"\"Convert the list of SAE tweets provided to convert to African American English.\"\"\"\n",
    "    aae_from_sae_tweets: List[StandardEnglish] = Field(description=\"The list of converted tweets by order of the sentences given.\")\n",
    "\n",
    "\n",
    "aae_from_sae_chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You will be given a list of tweets in Standard American English. Your task is to convert the given tweets to African American English.\"\"\"\n",
    "        ),\n",
    "        (\"user\", \"{sentences}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatAnthropic(model=\"claude-3-haiku-20240307\", timeout=None,\n",
    "    max_retries=2, temperature=0)\n",
    "\n",
    "\n",
    "aae_from_sae_runnable = aae_from_sae_chat_template | model.with_structured_output(schema=Data)\n",
    "\n",
    "chat_template_single = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You will be given a tweet in Standard American English. Your task is to convert the given tweet to African American English.\"\"\"\n",
    "        ),\n",
    "        (\"user\", \"{sentences}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "aae_from_sae_runnable_single = chat_template_single | model.with_structured_output(schema=StandardEnglish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6bfbcfec-55ed-469a-9970-7ec4a8d878f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_dataset = pd.read_csv('./labeled/Claude-Haiku-sae-labels.csv')[\"standard_american_english\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "921fd322-866d-46af-90c9-f97831d2f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_aae_from_sae_sentence = []\n",
    "processed_indices = []\n",
    "failed_indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d12360ec-9df5-4d78-a07b-954deca68ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▍                                                                                                                                                                                                               | 4/600 [00:08<20:09,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing batch 15-19: 5 validation errors for Data\n",
      "aae_from_sae_tweets -> 0\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "aae_from_sae_tweets -> 1\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "aae_from_sae_tweets -> 2\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "aae_from_sae_tweets -> 3\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "aae_from_sae_tweets -> 4\n",
      "  value is not a valid dict (type=type_error.dict)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██▍                                                                                                                                                                                                              | 7/600 [00:13<17:58,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing batch 30-34: 5 validation errors for Data\n",
      "aae_from_sae_tweets -> 0\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "aae_from_sae_tweets -> 1\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "aae_from_sae_tweets -> 2\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "aae_from_sae_tweets -> 3\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "aae_from_sae_tweets -> 4\n",
      "  value is not a valid dict (type=type_error.dict)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                              | 279/600 [09:23<10:01,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing batch 1390-1394: 5 validation errors for Data\n",
      "aae_from_sae_tweets -> 0\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "aae_from_sae_tweets -> 1\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "aae_from_sae_tweets -> 2\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "aae_from_sae_tweets -> 3\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "aae_from_sae_tweets -> 4\n",
      "  value is not a valid dict (type=type_error.dict)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|██████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                            | 285/600 [09:35<09:40,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing batch 1420-1424: 5 validation errors for Data\n",
      "aae_from_sae_tweets -> 0\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "aae_from_sae_tweets -> 1\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "aae_from_sae_tweets -> 2\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "aae_from_sae_tweets -> 3\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "aae_from_sae_tweets -> 4\n",
      "  value is not a valid dict (type=type_error.dict)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                   | 450/600 [15:09<04:35,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing batch 2245-2249: 5 validation errors for Data\n",
      "aae_from_sae_tweets -> 0\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "aae_from_sae_tweets -> 1\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "aae_from_sae_tweets -> 2\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "aae_from_sae_tweets -> 3\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "aae_from_sae_tweets -> 4\n",
      "  value is not a valid dict (type=type_error.dict)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 600/600 [20:18<00:00,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled dataset saved to ./labeled/Claude-Haiku-AAE_from_SAE.csv\n",
      "Failed indices saved to ./labeled/failed_indices_Claude-Haiku-AAE_from_SAE.csv\n",
      "Number of successfully processed sentences: 2770\n",
      "Number of failed sentences: 230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0, len(sae_dataset), 5)):\n",
    "    batch = sae_dataset[i:i+5].to_list()\n",
    "    batch_indices = list(range(i, min(i+5, len(sae_dataset))))\n",
    "    \n",
    "    try:\n",
    "        result = aae_from_sae_runnable.invoke({\"sentences\": \"\\n\".join(batch)})\n",
    "        \n",
    "        # Check if the number of returned sentiments matches the batch size\n",
    "        if len(result.aae_from_sae_tweets) == len(batch):\n",
    "            all_aae_from_sae_sentence.extend([response.african_american_english for response in result.aae_from_sae_tweets])\n",
    "            processed_indices.extend(batch_indices)\n",
    "        else:\n",
    "            # If the number of sentiments doesn't match, mark all as failed\n",
    "            failed_indices.extend(batch_indices)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {i}-{i+4}: {str(e)}\")\n",
    "        failed_indices.extend(batch_indices)\n",
    "\n",
    "# Create a new dataframe with successfully processed sentences and sentiments\n",
    "labeled_df = pd.DataFrame({\n",
    "    'index': processed_indices,\n",
    "    'standard_american_english' : sae_dataset.iloc[processed_indices],\n",
    "    'aae_from_sae': all_aae_from_sae_sentence\n",
    "})\n",
    "\n",
    "# Sort the dataframe by the original index\n",
    "labeled_df = labeled_df.sort_values('index').reset_index(drop=True)\n",
    "\n",
    "# Save the labeled dataset to a CSV file\n",
    "output_path = './labeled/Claude-Haiku-AAE_from_SAE.csv'\n",
    "labeled_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Labeled dataset saved to {output_path}\")\n",
    "\n",
    "# Save the failed indices to a separate file\n",
    "failed_indices_path = './labeled/failed_indices_Claude-Haiku-AAE_from_SAE.csv'\n",
    "pd.DataFrame({'failed_index': failed_indices}).to_csv(failed_indices_path, index=False)\n",
    "\n",
    "print(f\"Failed indices saved to {failed_indices_path}\")\n",
    "print(f\"Number of successfully processed sentences: {len(processed_indices)}\")\n",
    "print(f\"Number of failed sentences: {len(failed_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9c52b6d4-c21b-467e-aab9-dadcaad5ada3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 115/115 [02:39<00:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Claude-Haiku-AAE_from_SAE dataset saved to ./labeled/complete-3000-Claude-Haiku-AAE_from_SAE.csv\n",
      "Total processed sentences: 3000\n",
      "Still Failed sentences: 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labeled_df = pd.read_csv('./labeled/Claude-Haiku-AAE_from_SAE.csv')\n",
    "failed_indices = pd.read_csv('./labeled/failed_indices_Claude-Haiku-AAE_from_SAE.csv')['failed_index'].tolist()\n",
    "\n",
    "\n",
    "new_aae_from_sae_sentence = []\n",
    "new_processed_indices = []\n",
    "still_failed_indices = []\n",
    "\n",
    "for i in tqdm(range(0, len(failed_indices), 2)):\n",
    "    batch_indices = failed_indices[i:i+2]\n",
    "    batch = sae_dataset.iloc[batch_indices].tolist()\n",
    "    try:\n",
    "        result = aae_from_sae_runnable.invoke({\"sentences\": \"\\n\".join(batch)})\n",
    "        \n",
    "        # Check if the number of returned sentiments matches the batch size\n",
    "        if len(result.aae_from_sae_tweets) == len(batch):\n",
    "            new_aae_from_sae_sentence.extend([response.african_american_english for response in result.aae_from_sae_tweets])\n",
    "            new_processed_indices.extend(batch_indices)\n",
    "        else:\n",
    "            # If the number of sentiments doesn't match, mark all as failed\n",
    "            still_failed_indices.extend(batch_indices)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {i}-{i+2}: {str(e)}\")\n",
    "        still_failed_indices.extend(batch_indices)\n",
    "\n",
    "# Create a new dataframe with newly processed sentences and sentiments\n",
    "new_labeled_df = pd.DataFrame({\n",
    "    'index': new_processed_indices,\n",
    "    'standard_american_english': sae_dataset.iloc[new_processed_indices],\n",
    "    'aae_from_sae': new_aae_from_sae_sentence\n",
    "})\n",
    "\n",
    "# Combine the previously processed data with the newly processed data\n",
    "combined_df = pd.concat([labeled_df, new_labeled_df], ignore_index=True)\n",
    "\n",
    "# Sort the dataframe by the original index and reset the index\n",
    "combined_df = combined_df.sort_values('index').reset_index(drop=True)\n",
    "\n",
    "# If there are still failed indices, add them to the combined dataframe with NaN sentiment\n",
    "if still_failed_indices:\n",
    "    failed_df = pd.DataFrame({\n",
    "        'index': still_failed_indices,\n",
    "        'standard_american_english': sae_dataset.iloc[still_failed_indices],\n",
    "        'aae_from_sae': pd.NA\n",
    "    })\n",
    "    combined_df = pd.concat([combined_df, failed_df], ignore_index=True)\n",
    "    combined_df = combined_df.sort_values('index').reset_index(drop=True)\n",
    "\n",
    "output_path = './labeled/complete-3000-Claude-Haiku-AAE_from_SAE.csv'\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Complete Claude-Haiku-AAE_from_SAE dataset saved to {output_path}\")\n",
    "print(f\"Total processed sentences: {len(combined_df)}\")\n",
    "print(f\"Still Failed sentences: {len(still_failed_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6f55df51-1603-4a72-9b4f-f2445d7303e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated complete labeled dataset saved to ./labeled/Claude-Haiku-AAE_from_SAE-FINAL.csv\n",
      "Total processed sentences: 3000\n",
      "Successfully labeled sentences: 3000\n",
      "Failed sentences: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "combined_df = pd.read_csv('./labeled/complete-3000-Claude-Haiku-AAE_from_SAE.csv')\n",
    "\n",
    "# Apply the lambda function to process the indices where sentiment is NaN\n",
    "combined_df['aae_from_sae'] = combined_df.apply(\n",
    "    lambda row: aae_from_sae_runnable_single.invoke({\"sentences\": row['standard_american_english']}).african_american_english\n",
    "    if pd.isna(row['aae_from_sae']) else row['aae_from_sae'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save the updated complete labeled dataset\n",
    "output_path = './labeled/Claude-Haiku-AAE_from_SAE-FINAL.csv'\n",
    "combined_df[[\"aae_from_sae\", \"standard_american_english\"]].to_csv(output_path, index=False)\n",
    "\n",
    "# Display the summary of the updated dataset\n",
    "updated_total = len(combined_df)\n",
    "updated_successful = combined_df['aae_from_sae'].notna().sum()\n",
    "updated_failed = combined_df['aae_from_sae'].isna().sum()\n",
    "\n",
    "print(f\"Updated complete labeled dataset saved to {output_path}\")\n",
    "print(f\"Total processed sentences: {updated_total}\")\n",
    "print(f\"Successfully labeled sentences: {updated_successful}\")\n",
    "print(f\"Failed sentences: {updated_failed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f6a071-a03a-48b5-b49a-1a75b726995f",
   "metadata": {},
   "source": [
    "## Getting the sentiment for AAE_from_SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9e78df00-b954-40d3-9e41-acd4a34c6b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for sentiment analysis\n",
    "class SentimentAnalysisResponse(BaseModel):\n",
    "    sentiment: str = Field(description=\"The sentiment of the sentence (Positive, Negative, or Neutral)\")\n",
    "\n",
    "class Data(BaseModel):\n",
    "    \"\"\"Extracted data about sentences.\"\"\"\n",
    "    sentiments: List[SentimentAnalysisResponse]\n",
    "\n",
    "# Define the prompt template\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Your task is to analyze the provided sentences written in African American English and identify the sentiment expressed by the author. \n",
    "            The sentiment should be classified as Positive, Negative, or Neutral for each sentence.\"\"\"\n",
    "        ),\n",
    "        (\"user\", \"{sentences}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatAnthropic(model=\"claude-3-haiku-20240307\", timeout=None,\n",
    "    max_retries=2, temperature=0)\n",
    "\n",
    "# Create the runnable chain\n",
    "runnable = chat_template | model.with_structured_output(schema=Data)\n",
    "\n",
    "single_chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Your task is to analyze the provided sentence written in African American English and identify the sentiment expressed by the author. \n",
    "            The sentiment should be classified as Positive, Negative, or Neutral for each sentence.\"\"\"\n",
    "        ),\n",
    "        (\"user\", \"{sentences}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "runnable_single = single_chat_template | model.with_structured_output(schema=SentimentAnalysisResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bfb5a3a7-a73f-4cf0-841b-1b51f7253e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./labeled/Claude-Haiku-AAE_from_SAE.csv\")[\"aae_from_sae\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fed7e1cd-b8c9-4cd8-a2eb-6b09617557d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 600/600 [10:33<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled dataset saved to ./labeled/Claude-Haiku-AAE_from_SAE_labels.csv\n",
      "Failed indices saved to ./labeled/Claude-Haiku-AAE_from_SAE_labels_failed_indices.csv\n",
      "Number of successfully processed sentences: 2910\n",
      "Number of failed sentences: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store the sentiments and their corresponding indices\n",
    "all_sentiments = []\n",
    "processed_indices = []\n",
    "failed_indices = []\n",
    "\n",
    "# Process the dataset in batches of 5\n",
    "for i in tqdm(range(0, len(dataset), 5)):\n",
    "    batch = dataset[i:i+5].to_list()\n",
    "    batch_indices = list(range(i, min(i+5, len(dataset))))\n",
    "    \n",
    "    try:\n",
    "        result = runnable.invoke({\"sentences\": \"\\n\".join(batch)})\n",
    "        \n",
    "        # Check if the number of returned sentiments matches the batch size\n",
    "        if len(result.sentiments) == len(batch):\n",
    "            all_sentiments.extend([response.sentiment for response in result.sentiments])\n",
    "            processed_indices.extend(batch_indices)\n",
    "        else:\n",
    "            # If the number of sentiments doesn't match, mark all as failed\n",
    "            failed_indices.extend(batch_indices)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {i}-{i+4}: {str(e)}\")\n",
    "        failed_indices.extend(batch_indices)\n",
    "\n",
    "# Create a new dataframe with successfully processed sentences and sentiments\n",
    "labeled_df = pd.DataFrame({\n",
    "    'index': processed_indices,\n",
    "    'aae_from_sae': dataset.iloc[processed_indices],\n",
    "    'sentiment': all_sentiments\n",
    "})\n",
    "\n",
    "# Sort the dataframe by the original index\n",
    "labeled_df = labeled_df.sort_values('index').reset_index(drop=True)\n",
    "\n",
    "# Save the labeled dataset to a CSV file\n",
    "output_path = './labeled/Claude-Haiku-AAE_from_SAE_labels.csv'\n",
    "labeled_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Labeled dataset saved to {output_path}\")\n",
    "\n",
    "# Save the failed indices to a separate file\n",
    "failed_indices_path = './labeled/Claude-Haiku-AAE_from_SAE_labels_failed_indices.csv'\n",
    "pd.DataFrame({'failed_index': failed_indices}).to_csv(failed_indices_path, index=False)\n",
    "\n",
    "print(f\"Failed indices saved to {failed_indices_path}\")\n",
    "print(f\"Number of successfully processed sentences: {len(processed_indices)}\")\n",
    "print(f\"Number of failed sentences: {len(failed_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "39847b3b-3800-402e-963c-cfe6201ebdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [00:48<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete labeled dataset saved to ./labeled/complete-3000-Claude-Haiku-AAE_from_SAE_labels.csv\n",
      "Total processed sentences: 3000\n",
      "Successfully labeled sentences: 2956\n",
      "Failed sentences: 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the previously processed data\n",
    "labeled_df = pd.read_csv('./labeled/Claude-Haiku-AAE_from_SAE_labels.csv')\n",
    "failed_indices = pd.read_csv('./labeled/Claude-Haiku-AAE_from_SAE_labels_failed_indices.csv')['failed_index'].tolist()\n",
    "\n",
    "\n",
    "# Initialize lists to store the new sentiments and their corresponding indices\n",
    "new_sentiments = []\n",
    "new_processed_indices = []\n",
    "still_failed_indices = []\n",
    "\n",
    "# Process the failed sentences\n",
    "for i in tqdm(range(0, len(failed_indices), 2)):\n",
    "    batch_indices = failed_indices[i:i+2]\n",
    "    batch = dataset.iloc[batch_indices].tolist()\n",
    "    \n",
    "    try:\n",
    "        result = runnable.invoke({\"sentences\": \"\\n\".join(batch)})\n",
    "        \n",
    "        # Check if the number of returned sentiments matches the batch size\n",
    "        if len(result.sentiments) == len(batch):\n",
    "            new_sentiments.extend([response.sentiment for response in result.sentiments])\n",
    "            new_processed_indices.extend(batch_indices)\n",
    "        else:\n",
    "            # If the number of sentiments doesn't match, mark all as still failed\n",
    "            still_failed_indices.extend(batch_indices)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {i}-{i+1}: {str(e)}\")\n",
    "        still_failed_indices.extend(batch_indices)\n",
    "\n",
    "# Create a new dataframe with newly processed sentences and sentiments\n",
    "new_labeled_df = pd.DataFrame({\n",
    "    'index': new_processed_indices,\n",
    "    'aae_from_sae': dataset.iloc[new_processed_indices],\n",
    "    'sentiment': new_sentiments\n",
    "})\n",
    "\n",
    "# Combine the previously processed data with the newly processed data\n",
    "combined_df = pd.concat([labeled_df, new_labeled_df], ignore_index=True)\n",
    "\n",
    "# Sort the dataframe by the original index and reset the index\n",
    "combined_df = combined_df.sort_values('index').reset_index(drop=True)\n",
    "\n",
    "# If there are still failed indices, add them to the combined dataframe with NaN sentiment\n",
    "if still_failed_indices:\n",
    "    failed_df = pd.DataFrame({\n",
    "        'index': still_failed_indices,\n",
    "        'aae_from_sae': dataset.iloc[still_failed_indices],\n",
    "        'sentiment': pd.NA\n",
    "    })\n",
    "    combined_df = pd.concat([combined_df, failed_df], ignore_index=True)\n",
    "    combined_df = combined_df.sort_values('index').reset_index(drop=True)\n",
    "\n",
    "# Save the complete labeled dataset to a CSV file\n",
    "output_path = './labeled/complete-3000-Claude-Haiku-AAE_from_SAE_labels.csv'\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Complete labeled dataset saved to {output_path}\")\n",
    "print(f\"Total processed sentences: {len(combined_df)}\")\n",
    "print(f\"Successfully labeled sentences: {combined_df['sentiment'].notna().sum()}\")\n",
    "print(f\"Failed sentences: {combined_df['sentiment'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8d18d2e9-84e0-45d0-8ec9-c334033ef88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated complete labeled dataset saved to ./labeled/complete-3000-Claude-Haiku-AAE_from_SAE_labels-final.csv\n",
      "Total processed sentences: 3000\n",
      "Successfully labeled sentences: 3000\n",
      "Failed sentences: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "combined_df = pd.read_csv('./labeled/complete-3000-Claude-Haiku-AAE_from_SAE_labels.csv')\n",
    "\n",
    "# Apply the lambda function to process the indices where sentiment is NaN\n",
    "combined_df['sentiment'] = combined_df.apply(\n",
    "    lambda row: runnable_single.invoke({\"sentences\": row['aae_from_sae']}).sentiment \n",
    "    if pd.isna(row['sentiment']) else row['sentiment'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save the updated complete labeled dataset\n",
    "output_path = './labeled/complete-3000-Claude-Haiku-AAE_from_SAE_labels-final.csv'\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Display the summary of the updated dataset\n",
    "updated_total = len(combined_df)\n",
    "updated_successful = combined_df['sentiment'].notna().sum()\n",
    "updated_failed = combined_df['sentiment'].isna().sum()\n",
    "\n",
    "print(f\"Updated complete labeled dataset saved to {output_path}\")\n",
    "print(f\"Total processed sentences: {updated_total}\")\n",
    "print(f\"Successfully labeled sentences: {updated_successful}\")\n",
    "print(f\"Failed sentences: {updated_failed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "db7fc895-7169-4642-be60-22499178e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = combined_df[[\"aae_from_sae\", \"sentiment\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fd3437c6-1cc3-4b95-873d-32b3d84e8781",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf1b06b-ceea-4988-ba9b-e90739a82721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "augment",
   "language": "python",
   "name": "augmentation-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
