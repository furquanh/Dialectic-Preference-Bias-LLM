{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bd14dec-276d-4124-b01d-3b8514c4ae1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c6f21c39f04a7ebdeb779de6ce82c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "cache_dir = \"../cache\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "\n",
    "# Load the Phi-3 model and tokenizer\n",
    "model_id = \"microsoft/Phi-3-medium-4k-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e26b8815-c552-468d-9b40-383164226ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ce2cb3570f4ffa95659e7ad5e18b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully created prompt column\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function generate_sentiment at 0x7f4ba4619b80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f37ee95d34444dabbaf587dd6e970b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully generated models answers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc643c245364b00842c99d49dd75a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully parsed sentiment out of each answer\n",
      "Labeled dataset saved to ./labeled/Phi-3-Labels.csv\n",
      "Number of processed sentences: 3000\n",
      "Number of Unknown sentiments: 0\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "datasets.logging.set_verbosity_info()\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./2000-5000_sentences.csv')\n",
    "\n",
    "# Function to create prompts for sentiment analysis\n",
    "def create_prompt(example):\n",
    "    return f\"\"\"<|user|>\n",
    "Your task is to analyze the provided sentences written in African American English and identify the sentiment expressed by the author. The sentiment should be classified as Positive, Negative, or Neutral. Reply with just the sentiment.\\n\n",
    "\"{example['text']}\"<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "# Create a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Add prompts to the dataset\n",
    "dataset = dataset.map(lambda example: {\"prompt\": create_prompt(example)})\n",
    "\n",
    "print(\"successfully created prompt column\")\n",
    "\n",
    "# Function to generate sentiments\n",
    "def generate_sentiment(examples):\n",
    "    outputs = pipe(\n",
    "        examples[\"prompt\"],\n",
    "        max_new_tokens=20,\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    return {\"generated_text\": [output[0][\"generated_text\"] for output in outputs]}\n",
    "\n",
    "# Generate sentiments\n",
    "dataset = dataset.map(\n",
    "    generate_sentiment,\n",
    "    batched=True,\n",
    "    batch_size=32,  # Adjust based on your GPU memory\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"successfully generated models answers\")\n",
    "\n",
    "# Function to extract sentiment from model output\n",
    "def extract_sentiment(output):\n",
    "    response = output.split(\"<|assistant|>\")[-1].strip()\n",
    "    if \"Positive\" in response:\n",
    "        return \"Positive\"\n",
    "    elif \"Negative\" in response:\n",
    "        return \"Negative\"\n",
    "    elif \"Neutral\" in response:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Extract sentiments\n",
    "dataset = dataset.map(lambda example: {\"sentiment\": extract_sentiment(example[\"generated_text\"])})\n",
    "\n",
    "print(\"successfully parsed sentiment out of each answer\")\n",
    "\n",
    "# Convert back to pandas DataFrame\n",
    "labeled_df = dataset.to_pandas()\n",
    "\n",
    "# Save the labeled dataset to a CSV file\n",
    "output_path = './labeled/Phi-3-Labels.csv'\n",
    "labeled_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Labeled dataset saved to {output_path}\")\n",
    "print(f\"Number of processed sentences: {len(labeled_df)}\")\n",
    "print(f\"Number of Unknown sentiments: {(labeled_df['sentiment'] == 'Unknown').sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81fcabb8-0bc7-494b-bce6-7932c036f65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@SeeLineWoman If I don't get this job tomorrow...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm going to try two-a-days, y'all. I want to ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I still intend to have his child, because he's...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Used someone as a reference with no warning. I...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@HumanistExec Try ginger for the nausea. Have ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  @SeeLineWoman If I don't get this job tomorrow...  Negative\n",
       "1  I'm going to try two-a-days, y'all. I want to ...   Neutral\n",
       "2  I still intend to have his child, because he's...  Positive\n",
       "3  Used someone as a reference with no warning. I...   Neutral\n",
       "4  @HumanistExec Try ginger for the nausea. Have ...   Neutral"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_and_sentiment = pd.DataFrame({\"text\" : df['text'], \"sentiment\" : dataset['sentiment']})\n",
    "text_and_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59f19562-e609-445c-b735-7e3686fd86f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = './labeled/Phi-3-Labels.csv'\n",
    "text_and_sentiment.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2a2d8ad-be77-47ea-b6cd-900ff94b4e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@SeeLineWoman If I don't get this job tomorrow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm going to try two-a-days, y'all. I want to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I still intend to have his child, because he's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Used someone as a reference with no warning. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@HumanistExec Try ginger for the nausea. Have ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  @SeeLineWoman If I don't get this job tomorrow...\n",
       "1  I'm going to try two-a-days, y'all. I want to ...\n",
       "2  I still intend to have his child, because he's...\n",
       "3  Used someone as a reference with no warning. I...\n",
       "4  @HumanistExec Try ginger for the nausea. Have ..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd04155-0ef1-4c3e-92ec-6ee44d916a9a",
   "metadata": {},
   "source": [
    "## Conversion to SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "102738ce-9db3-4a3c-863c-6abc521574c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32056d3071e4c0ab4fba7fb0fe50846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function generate_sae at 0x7f4b9805a5e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully created prompt column\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc1925ebfe54facafad8a004ba6708e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully generated models answers\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'set' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/local/home/furquanh/tmp/ipykernel_1177275/1620203379.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Save the labeled dataset to a CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./labeled/Phi-3-SAE.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mlabeled_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"standard_american_english\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"african_american_english\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"SAE dataset saved to {output_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mrename\u001b[0;34m(self, mapper, index, columns, axis, copy, inplace, level, errors)\u001b[0m\n\u001b[1;32m   5516\u001b[0m         \u001b[0;36m4\u001b[0m  \u001b[0;36m3\u001b[0m  \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5517\u001b[0m         \"\"\"\n\u001b[0;32m-> 5518\u001b[0;31m         return super()._rename(\n\u001b[0m\u001b[1;32m   5519\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5520\u001b[0m             \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_rename\u001b[0;34m(self, mapper, index, columns, axis, copy, inplace, level, errors)\u001b[0m\n\u001b[1;32m   1084\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{missing_labels} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m             \u001b[0mnew_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1087\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_axis_nocheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_transform_index\u001b[0;34m(self, func, level)\u001b[0m\n\u001b[1;32m   6462\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6463\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6464\u001b[0;31m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6465\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtupleize_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   6462\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6463\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6464\u001b[0;31m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6465\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtupleize_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'set' object is not callable"
     ]
    }
   ],
   "source": [
    "def create_sae_prompt(example):\n",
    "        return f\"\"\"<|user|>\n",
    "Following is a tweet extracted from a African American twitter individual's account. Your task is to convert the tweet to Standard American English. Reply with just the sentence.\n",
    "\"{example['text']}\"<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "    \n",
    "# Create a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Add prompts to the dataset\n",
    "dataset = dataset.map(lambda example: {\"prompt\": create_sae_prompt(example)})\n",
    "\n",
    "print(\"successfully created prompt column\")\n",
    "\n",
    "# Function to generate sentiments\n",
    "def generate_sae(examples):\n",
    "    outputs = pipe(\n",
    "        examples[\"prompt\"],\n",
    "        max_new_tokens=150,\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    return {\"standard_american_english\": [output[0][\"generated_text\"] for output in outputs]}\n",
    "\n",
    "# Generate sentiments\n",
    "dataset = dataset.map(\n",
    "    generate_sae,\n",
    "    batched=True,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(\"successfully generated models answers\")\n",
    "\n",
    "# Convert back to pandas DataFrame\n",
    "labeled_df = dataset.to_pandas()\n",
    "\n",
    "# Save the labeled dataset to a CSV file\n",
    "output_path = './labeled/Phi-3-SAE.csv'\n",
    "labeled_df[[\"text\", \"standard_american_english\"]].rename(columns={\"text\", \"african_american_english\"}).to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"SAE dataset saved to {output_path}\")\n",
    "print(f\"Number of processed sentences: {len(labeled_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be194518-a46b-44a5-8353-51a283ef088e",
   "metadata": {},
   "source": [
    "Suppose to be .rename(columns={\"text\" : \"african_american_english\"}) not .rename(columns={\"text\" , \"african_american_english\"})!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9db6042-ea4e-4402-9db0-c60295c11467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAE dataset saved to ./labeled/Phi-3-SAE.csv\n",
      "Number of processed sentences: 3000\n"
     ]
    }
   ],
   "source": [
    "# Save the labeled dataset to a CSV file\n",
    "output_path = './labeled/Phi-3-SAE.csv'\n",
    "labeled_df[[\"text\", \"standard_american_english\"]].rename(columns={\"text\" : \"african_american_english\"}).to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"SAE dataset saved to {output_path}\")\n",
    "print(f\"Number of processed sentences: {len(labeled_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a74a043-f693-431b-bc5c-b619fa7c2c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to pandas DataFrame\n",
    "labeled_df = dataset.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9916f6ab-7780-4ac2-accd-b3eecb589192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text                         @SeeLineWoman If I don't get this job tomorrow...\n",
      "prompt                       <|user|>\\nFollowing is a tweet extracted from ...\n",
      "standard_american_english    <|user|>\\nFollowing is a tweet extracted from ...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(labeled_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5251004a-e294-421a-8bee-0337ed539ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"@SeeLineWoman If I don't get this job tomorrow, I don't know what I'm going to do. I'm at the end of my rope.\",\n",
       " 'prompt': '<|user|>\\nFollowing is a tweet extracted from a African American twitter individual\\'s account. Your task is to convert the tweet to Standard American English. Reply with just the sentence.\\n\"@SeeLineWoman If I don\\'t get this job tomorrow, I don\\'t know what I\\'m going to do. I\\'m at the end of my rope.\"<|end|>\\n<|assistant|>',\n",
       " 'standard_american_english': '<|user|>\\nFollowing is a tweet extracted from a African American twitter individual\\'s account. Your task is to convert the tweet to Standard American English. Reply with just the sentence.\\n\"@SeeLineWoman If I don\\'t get this job tomorrow, I don\\'t know what I\\'m going to do. I\\'m at the end of my rope.\"<|end|>\\n<|assistant|> \"If I don\\'t get this job tomorrow, I don\\'t know what I\\'m going to do. I\\'m at the end of my rope.\"'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "774d9140-de21-4a4a-a47a-124887e440ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            african_american_english  \\\n",
      "0  @SeeLineWoman If I don't get this job tomorrow...   \n",
      "1  I'm going to try two-a-days, y'all. I want to ...   \n",
      "2  I still intend to have his child, because he's...   \n",
      "3  Used someone as a reference with no warning. I...   \n",
      "4  @HumanistExec Try ginger for the nausea. Have ...   \n",
      "\n",
      "                           standard_american_english  \n",
      "0  If I don't get this job tomorrow, I don't know...  \n",
      "1  I'm going to attempt double workouts, everyone...  \n",
      "2  I still plan to have his child, because he's a...  \n",
      "3  I used someone as a reference without giving a...  \n",
      "4  @HumanistExec Try ginger for the nausea. Have ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the labeled dataset\n",
    "labeled_df = pd.read_csv('./labeled/Phi-3-SAE.csv')\n",
    "\n",
    "def extract_sae(sentence):\n",
    "    try:\n",
    "        # Extract the part after the prompt and clean it\n",
    "        sae_sentence = sentence.split('<|assistant|>')[-1].strip().strip('\"')\n",
    "        return sae_sentence\n",
    "    except IndexError:\n",
    "        # Handle cases where the split might not work as expected\n",
    "        return sentence.strip().strip('\"')\n",
    "\n",
    "# Apply the extraction function to the 'standard_american_english' column\n",
    "labeled_df['standard_american_english'] = labeled_df['standard_american_english'].apply(extract_sae)\n",
    "\n",
    "# Save the cleaned dataset to a new CSV file\n",
    "output_path = './labeled/Phi-3-SAE-cleaned.csv'\n",
    "labeled_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(labeled_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f728a35b-6949-4bb9-8e37-43316a046db6",
   "metadata": {},
   "source": [
    "## SAE labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc9b2ca9-5a7b-4a61-be78-e5ed73f5b776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3925ef812ea143da9fe4026a25e47e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function generate_sae at 0x7f4ba436e160> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16b75f5d25a43fea8e98076f9e2fbe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAE dataset saved to ./labeled/Phi-3-SAE-Labels.csv\n",
      "Number of processed sentences: 3000\n",
      "Number of Unknown sentiments: 0\n"
     ]
    }
   ],
   "source": [
    "sae = pd.read_csv('./labeled/Phi-3-SAE-cleaned.csv')\n",
    "def create_sae_prompt(example):\n",
    "    return f\"\"\"<|user|>\n",
    "Your task is to analyze the provided sentences written in Standard American English and identify the sentiment expressed by the author. The sentiment should be classified as Positive, Negative, or Neutral for each sentence. Reply with just the sentiment.\\n\n",
    "\"{example['standard_american_english']}\"<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "    \n",
    "# Create a Hugging Face Dataset\n",
    "sae_labelsdataset = Dataset.from_pandas(sae)\n",
    "\n",
    "# Add prompts to the dataset\n",
    "sae_labelsdataset = sae_labelsdataset.map(lambda example: {\"prompt\": create_sae_prompt(example)})\n",
    "\n",
    "# Function to extract sentiment from model output\n",
    "def extract_sentiment(output):\n",
    "    response = output.split(\"<|assistant|>\")[-1].strip()\n",
    "    if \"Positive\" in response:\n",
    "        return \"Positive\"\n",
    "    elif \"Negative\" in response:\n",
    "        return \"Negative\"\n",
    "    elif \"Neutral\" in response:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    \n",
    "# Function to generate sentiments\n",
    "def generate_sae(examples):\n",
    "    outputs = pipe(\n",
    "        examples[\"prompt\"],\n",
    "        max_new_tokens=20,\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    return {\"sae_labels\": [extract_sentiment(output[0][\"generated_text\"]) for output in outputs]}\n",
    "\n",
    "# Generate sentiments\n",
    "sae_labelsdataset = sae_labelsdataset.map(\n",
    "    generate_sae,\n",
    "    batched=True,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "\n",
    "# Convert back to pandas DataFrame\n",
    "labeled_df_sae = sae_labelsdataset.to_pandas()\n",
    "\n",
    "\n",
    "# Save the labeled dataset to a CSV file\n",
    "output_path = './labeled/Phi-3-SAE-Labels.csv'\n",
    "labeled_df_sae[[\"standard_american_english\", \"sae_labels\"]].to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"SAE dataset saved to {output_path}\")\n",
    "print(f\"Number of processed sentences: {len(labeled_df_sae)}\")\n",
    "print(f\"Number of Unknown sentiments: {(labeled_df_sae['sae_labels'] == 'Unknown').sum()}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d619997b-79b7-4b55-8c98-ca4f7c545607",
   "metadata": {},
   "source": [
    "________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69318da6-524f-4292-a25c-51d7f55f67d0",
   "metadata": {},
   "source": [
    "### Pipiline uptil now:\n",
    "AAE -> AAE Sentiment -> Translate to SAE -> SAE Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1f2516-48a3-4f09-b0de-33743fcb0c64",
   "metadata": {},
   "source": [
    "## Now we go back to AAE using SAE, and finish off by obtaining sentiment on that.\n",
    "\n",
    "AAE -> AAE Sentiment -> Translate to SAE -> SAE Sentiment -> **AAE_from_SAE -> AAE_from_SAE Sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fa94575-981d-4d7e-aa16-281c8e43b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./labeled/Phi-3-SAE-Labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86e515c6-192e-4442-90ce-97868262bc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b507a52ec793462a9acb6e829327b7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully created prompt column\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function generate_aae_from_sae at 0x7f382c693160> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8532b846cc54cc89d3ebf8f83548103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully generated models answers\n",
      "AAE_from_SAE dataset saved to ./labeled/Phi-3-AAE_from_SAE.csv\n",
      "Number of processed sentences: 3000\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "datasets.logging.set_verbosity_info()\n",
    "\n",
    "def create_aae_from_sae_prompt(example):\n",
    "        return f\"\"\"<|user|>\n",
    "You will be given a tweet in Standard American English. Your task is to convert the given tweet to African American English. Reply with just the translated sentence.\n",
    "\"{example['standard_american_english']}\"<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "    \n",
    "# Create a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Add prompts to the dataset\n",
    "dataset = dataset.map(lambda example: {\"prompt\": create_aae_from_sae_prompt(example)})\n",
    "\n",
    "print(\"successfully created prompt column\")\n",
    "\n",
    "def extract_aae_from_sae(sentence):\n",
    "    try:\n",
    "        sae_sentence = sentence.split('<|assistant|>')[-1].strip().strip('\"')\n",
    "        return sae_sentence\n",
    "    except IndexError:\n",
    "        return sentence.strip().strip('\"')\n",
    "\n",
    "# Function to generate sentiments\n",
    "def generate_aae_from_sae(examples):\n",
    "    outputs = pipe(\n",
    "        examples[\"prompt\"],\n",
    "        max_new_tokens=150,\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    return {\"AAE_from_SAE\": [extract_aae_from_sae(output[0][\"generated_text\"]) for output in outputs]}\n",
    "\n",
    "# Generate AAE from SAE sentences\n",
    "dataset = dataset.map(\n",
    "    generate_aae_from_sae,\n",
    "    batched=True,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(\"successfully generated models answers\")\n",
    "\n",
    "# Convert back to pandas DataFrame\n",
    "labeled_df = dataset.to_pandas()\n",
    "\n",
    "\n",
    "output_path = './labeled/Phi-3-AAE_from_SAE.csv'\n",
    "labeled_df[[\"AAE_from_SAE\", \"standard_american_english\"]].to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"AAE_from_SAE dataset saved to {output_path}\")\n",
    "print(f\"Number of processed sentences: {len(labeled_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c661a06-070b-44cd-bffd-1d4a7326ae62",
   "metadata": {},
   "source": [
    "## Getting the sentiment for AAE_from_SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c94a5849-36ad-48cc-8f55-bf65e327d191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f739e8f3e5477fa648b3bdeac32d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function generate_aae_from_sae_sentiment at 0x7f384807c280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully created prompt column\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e157597e7e744324bd1133055614da38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully generated models answers\n",
      "AAE_from_SAE dataset saved to ./labeled/Phi-3-AAE_from_SAE_labels.csv\n",
      "Number of processed sentences: 3000\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "datasets.logging.set_verbosity_info()\n",
    "\n",
    "df = pd.read_csv('./labeled/Phi-3-AAE_from_SAE.csv')\n",
    "\n",
    "def create_aae_from_sae_prompt(example):\n",
    "        return f\"\"\"<|user|>\n",
    "Your task is to analyze the provided sentences written in African American English and identify the sentiment expressed by the author. The sentiment should be classified as Positive, Negative, or Neutral.\n",
    "\"{example['AAE_from_SAE']}\"<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "    \n",
    "# Create a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Add prompts to the dataset\n",
    "dataset = dataset.map(lambda example: {\"prompt\": create_aae_from_sae_prompt(example)})\n",
    "\n",
    "print(\"successfully created prompt column\")\n",
    "\n",
    "# Function to extract sentiment from model output\n",
    "def extract_sentiment(output):\n",
    "    response = output.split(\"<|assistant|>\")[-1].strip()\n",
    "    if \"Positive\" in response:\n",
    "        return \"Positive\"\n",
    "    elif \"Negative\" in response:\n",
    "        return \"Negative\"\n",
    "    elif \"Neutral\" in response:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "\n",
    "# Function to generate sentiments\n",
    "def generate_aae_from_sae_sentiment(examples):\n",
    "    outputs = pipe(\n",
    "        examples[\"prompt\"],\n",
    "        max_new_tokens=20,\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    return {\"AAE_from_SAE sentiment\": [extract_sentiment(output[0][\"generated_text\"]) for output in outputs]}\n",
    "\n",
    "# Generate AAE from SAE sentences\n",
    "dataset = dataset.map(\n",
    "    generate_aae_from_sae_sentiment,\n",
    "    batched=True,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(\"successfully generated models answers\")\n",
    "\n",
    "# Convert back to pandas DataFrame\n",
    "labeled_df = dataset.to_pandas()\n",
    "\n",
    "\n",
    "output_path = './labeled/Phi-3-AAE_from_SAE_labels.csv'\n",
    "labeled_df[[\"AAE_from_SAE\", \"AAE_from_SAE sentiment\"]].to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"AAE_from_SAE dataset saved to {output_path}\")\n",
    "print(f\"Number of processed sentences: {len(labeled_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c05486-e562-4e83-bcbb-6049ac0ea2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "augment",
   "language": "python",
   "name": "augmentation-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
