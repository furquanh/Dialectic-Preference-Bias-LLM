{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd14dec-276d-4124-b01d-3b8514c4ae1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d90633cbf0486e83f3051191e9d299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f817c4ba30b4404b3e00df67e2a56f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b35d65cd6414b28862b2d3d145890e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61adb5c4032d494da401db494ebce4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e425f296bd54597bc6b70a7852588bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ad8b4d31174989b1a31b6e46846c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/568 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "cache_dir = \"./cache/\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "\n",
    "# Load the Phi-3 model and tokenizer\n",
    "model_id = \"microsoft/Phi-3-medium-4k-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cuda:0\", \n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the dataset\n",
    "#dataset = pd.read_csv('./labeled/Phi-3-sae.csv')[\"standard_american_english\"]\n",
    "dataset = pd.read_csv('./labeled/Phi-3-AAE_from_SAE.csv')[\"aae_from_sae\"]\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"You will be given a list of tweets in Standard American English. Your task is to convert the given tweets to African American English.\"}\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=100,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c8f4d6e-c9b6-4df4-b8fa-efc78faec018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from typing import List, Optional\n",
    "from tqdm import tqdm\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "hf = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6c91063-bb7b-4ed6-998e-a5e90949998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "    ResponseSchema(name=\"aae_english\", description=\"The tweet converted into African American English\")]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8ea9e48-9c39-42df-aead-fb772d304127",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAE(BaseModel):\n",
    "    aae_english: str = Field(description=\"The tweet converted into African American English.\")\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You will be given a tweet in Standard American English. Your task is to convert the given tweet to African American English. Reply with just the converted tweet.\"\"\"\n",
    "        ),\n",
    "        (\"user\", \"{sentence}\\nAssistant: \")\n",
    "    ]\n",
    ")\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"System: You will be given a tweet in Standard American English. Your task is to convert the given tweet to African American English. Reply with just the converted tweet.\\n{format_instructions}\\nHuman: {sentence}\",\n",
    "    input_variables=[\"sentence\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5e97478-c5cd-43e3-b997-54a4bc3966a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You will be given a tweet in Standard American English. Your task is to convert the given tweet to African American English. Reply with just the converted tweet.\n",
      "Human: I'm the type that would walk through the fire to check the way it burns! \n",
      "Assistant: \n",
      "\n",
      "I be the type dat would walk through de fire to check how it burnin'!\n",
      "\n",
      "\n",
      "Human: I'm the type that would walk through the fire to check the way it burns! \n",
      "Assistant: \n",
      "\n",
      "I be the type dat would walk through de fire to check how it burnin'!\n",
      "\n",
      "\n",
      "Human: I'm the type that would walk through the fire to check the way it burns! \n",
      "Assistant\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "chain = chat_template | hf.bind(skip_prompt=True)  \n",
    "\n",
    "question = \"I'm the type that would walk through the fire to check the way it burns! \"\n",
    "\n",
    "print(chain.invoke({\"sentence\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74684a81-5c0c-490e-81dd-c480c486e92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = {\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"return_full_text\": False,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "# Function to get sentiment using Phi-3\n",
    "def get_aae(sentence):\n",
    "#     prompt = f\"\"\"<|user|>\n",
    "# You will be given a tweet in Standard American English. Your task is to convert the given tweet to African American English. Reply with just the converted tweet.\\n\"{sentence}\"<|end|>\n",
    "# <|assistant|>\"\"\"\n",
    "    \n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model.generate(**inputs, max_new_tokens=100, num_return_sequences=1)\n",
    "#     response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # return response\n",
    "    \n",
    "    messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"You will be given a tweet in Standard American English. Your task is to convert the given tweet to African American English. Reply with just the converted tweet.\\n'{sentence}'\"\"\"}\n",
    "    ]\n",
    "    output = pipe(messages, **generation_args)\n",
    "    return output[0]['generated_text']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21997799-5161-4b3c-80a6-927e0563de0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"Ain't nothin' but a breeze today, let's hit up them eats!\"\n"
     ]
    }
   ],
   "source": [
    "print(get_aae(\"The weather is lovely today, let's go out to eat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6aacb30c-d882-4fe1-94ea-96c82a5e1731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       She can't get anything from me, but bubble gum...\n",
       "1       @islandboi_B Yes, that's what's up. Nothing li...\n",
       "2       Mixed, huh! Those prominent knees and elbows w...\n",
       "3       The player Mike James from the Mavs is not imp...\n",
       "4       It took a whole stranger to tell me he is prou...\n",
       "                              ...                        \n",
       "1995    @NerdLifeThugging: Spending that kind of time ...\n",
       "1996    @MonsieurBLVD If it was a leather item and the...\n",
       "1997    @drkwingduck You have to stick with it for a f...\n",
       "1998    I feel like she thinks that's what I'm saying,...\n",
       "1999    @SeeLineWoman As someone who has always had so...\n",
       "Name: standard_american_english, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24fc283e-91f2-4503-8e13-6996d36a07f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bc85deb1c648f19a912edd2f1427fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function generate_aae at 0x7f69243cfaf0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e5d7c267bf40b18a7b5f49ec89fe39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a527596ef24a95a16da5cbc39c2730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "\"['standard_american_english'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/local/home/furquanh/tmp/ipykernel_464927/4198075081.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# Save the labeled dataset to a CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./labeled/Phi-3-AAE_from_SAE.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mlabeled_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"standard_american_english\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"aae_from_sae\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Labeled dataset saved to {output_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3897\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3898\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3899\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3901\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6112\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6116\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6177\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6178\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6180\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['standard_american_english'] not in index\""
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv('./labeled/Phi-3-sae.csv')\n",
    "\n",
    "# Function to create prompts for AAE conversion\n",
    "def create_prompt(example):\n",
    "    return f\"\"\"<|user|>\n",
    "You will be given a tweet in Standard American English. Your task is to convert the given tweet to African American English. Reply with just the converted tweet.\n",
    "\n",
    "'{example['standard_american_english']}'<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "# Create a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Add prompts to the dataset\n",
    "dataset = dataset.map(lambda example: {\"prompt\": create_prompt(example)})\n",
    "\n",
    "# Function to generate AAE sentences\n",
    "def generate_aae(examples):\n",
    "    outputs = pipe(\n",
    "        examples[\"prompt\"],\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    return {\"generated_text\": [output[0][\"generated_text\"] for output in outputs]}\n",
    "\n",
    "# Generate AAE sentences\n",
    "dataset = dataset.map(\n",
    "    generate_aae,\n",
    "    batched=True,\n",
    "    batch_size=32,  # Adjust based on your GPU memory\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "# Function to extract AAE sentence from model output\n",
    "def extract_aae(output):\n",
    "    return output.split(\"<|assistant|>\")[-1].strip()\n",
    "\n",
    "# Extract AAE sentences\n",
    "dataset = dataset.map(lambda example: {\"aae_from_sae\": extract_aae(example[\"generated_text\"])})\n",
    "\n",
    "# Convert back to pandas DataFrame\n",
    "labeled_df = dataset.to_pandas()\n",
    "\n",
    "# Save the labeled dataset to a CSV file\n",
    "output_path = './labeled/Phi-3-AAE_from_SAE.csv'\n",
    "labeled_df[[\"standard_american_english\", \"aae_from_sae\"]].to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Labeled dataset saved to {output_path}\")\n",
    "print(f\"Number of processed sentences: {len(labeled_df)}\")\n",
    "print(f\"Number of empty conversions: {labeled_df['african_american_english'].isna().sum()}\")\n",
    "\n",
    "# Clear CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dfc24a54-5e8d-4722-b0c0-fe7d491db521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled dataset saved to ./labeled/Phi-3-AAE_from_SAE-after-error.csv\n",
      "Number of processed sentences: 2000\n",
      "Number of empty conversions: 0\n"
     ]
    }
   ],
   "source": [
    "# Save the labeled dataset to a CSV file\n",
    "output_path = './labeled/Phi-3-AAE_from_SAE-after-error.csv'\n",
    "labeled_df['standard_american_english'] = df['standard_american_english']\n",
    "labeled_df[[\"standard_american_english\", \"aae_from_sae\"]].to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Labeled dataset saved to {output_path}\")\n",
    "print(f\"Number of processed sentences: {len(labeled_df)}\")\n",
    "print(f\"Number of empty conversions: {labeled_df['aae_from_sae'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e26b8815-c552-468d-9b40-383164226ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295b566b42a748fa9af00cd0a23e372f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function generate_sentiment at 0x7f0f40539820> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5132af6ccd55419ea01260a86895074a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983b4a6c66d24ff595bb32e1cd4e2938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled dataset saved to ./labeled/Phi-3-Labels.csv\n",
      "Number of processed sentences: 2000\n",
      "Number of Unknown sentiments: 2000\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./labeled/Phi-3-AAE_from_SAE.csv')\n",
    "\n",
    "# Function to create prompts for sentiment analysis\n",
    "def create_prompt(example):\n",
    "    return f\"\"\"<|user|>\n",
    "Your task is to analyze the provided sentences written in African American English and identify the sentiment expressed by the author. The sentiment should be classified as Positive, Negative, or Neutral for each sentence. Reply with just the sentiment.\\n\n",
    "\"{example['aae_from_sae']}\"<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "# Create a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Add prompts to the dataset\n",
    "dataset = dataset.map(lambda example: {\"prompt\": create_prompt(example)})\n",
    "\n",
    "# Function to generate sentiments\n",
    "def generate_sentiment(examples):\n",
    "    outputs = pipe(\n",
    "        examples[\"prompt\"],\n",
    "        max_new_tokens=20,\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    return {\"generated_text\": [output[0][\"generated_text\"] for output in outputs]}\n",
    "\n",
    "# Generate sentiments\n",
    "dataset = dataset.map(\n",
    "    generate_sentiment,\n",
    "    batched=True,\n",
    "    batch_size=32,  # Adjust based on your GPU memory\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "# Function to extract sentiment from model output\n",
    "def extract_sentiment(output):\n",
    "    response = output.split(\"<|assistant|>\")[-1].strip()\n",
    "    if \"Positive\" in response:\n",
    "        return \"Positive\"\n",
    "    elif \"Negative\" in response:\n",
    "        return \"Negative\"\n",
    "    elif \"Neutral\" in response:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Extract sentiments\n",
    "dataset = dataset.map(lambda example: {\"sentiment\": extract_sentiment(example[\"generated_text\"])})\n",
    "\n",
    "# Convert back to pandas DataFrame\n",
    "labeled_df = dataset.to_pandas()\n",
    "\n",
    "# Save the labeled dataset to a CSV file\n",
    "output_path = './labeled/Phi-3-AAE_from_SAE_Labels.csv'\n",
    "labeled_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Labeled dataset saved to {output_path}\")\n",
    "print(f\"Number of processed sentences: {len(labeled_df)}\")\n",
    "print(f\"Number of Unknown sentiments: {(labeled_df['sentiment'] == 'Unknown').sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9fb4a7d-9846-461e-9423-fae7e7840a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d361e54c4a44947882ba23d85729687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled dataset saved to ./labeled/Phi-3-Labels.csv\n",
      "Number of processed sentences: 2000\n",
      "Number of Unknown sentiments: 0\n"
     ]
    }
   ],
   "source": [
    "# Function to extract sentiment from model output\n",
    "def extract_sentiment(output):\n",
    "    response = output.split(\"<|assistant|>\")[-1].strip()\n",
    "    if \"Positive\" in response:\n",
    "        return \"Positive\"\n",
    "    elif \"Negative\" in response:\n",
    "        return \"Negative\"\n",
    "    elif \"Neutral\" in response:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Extract sentiments\n",
    "dataset = dataset.map(lambda example: {\"sentiment\": extract_sentiment(example[\"generated_text\"])})\n",
    "\n",
    "# Convert back to pandas DataFrame\n",
    "labeled_df = dataset.to_pandas()\n",
    "\n",
    "# Save the labeled dataset to a CSV file\n",
    "output_path = './labeled/Phi-3-Labels.csv'\n",
    "labeled_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Labeled dataset saved to {output_path}\")\n",
    "print(f\"Number of processed sentences: {len(labeled_df)}\")\n",
    "print(f\"Number of Unknown sentiments: {(labeled_df['sentiment'] == 'Unknown').sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f24d913-23f1-4119-bec9-b30481ad418a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generated_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>aae_from_sae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|user|&gt;\\nYour task is to analyze the provided...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>She can't get nothin' from me, but bubble gum ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;|user|&gt;\\nYour task is to analyze the provided...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>\"@islandboi_B Yo, that's what's up. Nothin' li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;|user|&gt;\\nYour task is to analyze the provided...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>'Mixed, huh! Them big knees and elbows gon' sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;|user|&gt;\\nYour task is to analyze the provided...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>\"Dat player Mike James from dey Mavs ain't imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;|user|&gt;\\nYour task is to analyze the provided...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>It took a whole stranger to tell me he proud o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      generated_text sentiment  \\\n",
       "0  <|user|>\\nYour task is to analyze the provided...  Negative   \n",
       "1  <|user|>\\nYour task is to analyze the provided...  Positive   \n",
       "2  <|user|>\\nYour task is to analyze the provided...  Negative   \n",
       "3  <|user|>\\nYour task is to analyze the provided...  Negative   \n",
       "4  <|user|>\\nYour task is to analyze the provided...  Positive   \n",
       "\n",
       "                                        aae_from_sae  \n",
       "0  She can't get nothin' from me, but bubble gum ...  \n",
       "1  \"@islandboi_B Yo, that's what's up. Nothin' li...  \n",
       "2  'Mixed, huh! Them big knees and elbows gon' sh...  \n",
       "3  \"Dat player Mike James from dey Mavs ain't imp...  \n",
       "4  It took a whole stranger to tell me he proud o...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_df['aae_from_sae'] = df['aae_from_sae']\n",
    "labeled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cce52e6-5429-4714-920b-62b7a7d5a995",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = './labeled/Phi-3-Labels.csv'\n",
    "labeled_df[[\"aae_from_sae\", \"sentiment\"]].to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf79c303-db42-4869-bfc6-73ba21411d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Your task is to analyze the provided sentences written in African American English and identify the sentiment expressed by the author. The sentiment should be classified as Positive, Negative, or Neutral for each sentence. Reply with just the sentiment.\n",
      "\n",
      "\"i spent half of my life running frm niggas dat were criminals not knowin dat da dude i thought was legit was a criminal smfh\"<|end|>\n",
      "<|assistant|> Negative\n"
     ]
    }
   ],
   "source": [
    "def create_prompt_single(sentence):\n",
    "    return f\"\"\"<|user|>\n",
    "Your task is to analyze the provided sentences written in African American English and identify the sentiment expressed by the author. The sentiment should be classified as Positive, Negative, or Neutral for each sentence. Reply with just the sentiment.\\n\n",
    "\"{sentence}\"<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "print(pipe(\n",
    "        create_prompt_single(\"i spent half of my life running frm niggas dat were criminals not knowin dat da dude i thought was legit was a criminal smfh\"),\n",
    "        max_new_tokens=10,\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1\n",
    "    )[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd04155-0ef1-4c3e-92ec-6ee44d916a9a",
   "metadata": {},
   "source": [
    "## Conversion to SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "102738ce-9db3-4a3c-863c-6abc521574c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d8bfea33514cc79f0607d0442e4c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00365106799b45eeb1bc830e59f6599e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAE dataset saved to ./labeled/Phi-3-SAE.csv\n",
      "Number of processed sentences: 2000\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3790\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3791\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sentiment'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/local/home/furquanh/tmp/ipykernel_1321427/1568504372.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"SAE dataset saved to {output_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Number of processed sentences: {len(labeled_df)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Number of Unknown sentiments: {(labeled_df['sentiment'] == 'Unknown').sum()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3892\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3893\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3894\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3895\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3795\u001b[0m             ):\n\u001b[1;32m   3796\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3797\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3798\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3799\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sentiment'"
     ]
    }
   ],
   "source": [
    "def create_sae_prompt(example):\n",
    "        return f\"\"\"<|user|>\n",
    "Following is a tweet extracted from a African American twitter individual's account. Your task is to convert the tweet to Standard American English. Reply with just the sentence.\n",
    "\"{example['text']}\"<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "    \n",
    "# Create a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Add prompts to the dataset\n",
    "dataset = dataset.map(lambda example: {\"prompt\": create_sae_prompt(example)})\n",
    "\n",
    "# Function to generate sentiments\n",
    "def generate_sae(examples):\n",
    "    outputs = pipe(\n",
    "        examples[\"prompt\"],\n",
    "        max_new_tokens=150,\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    return {\"standard_american_english\": [output[0][\"generated_text\"] for output in outputs]}\n",
    "\n",
    "# Generate sentiments\n",
    "dataset = dataset.map(\n",
    "    generate_sae,\n",
    "    batched=True,\n",
    "    batch_size=32,  # Adjust based on your GPU memory\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "# Convert back to pandas DataFrame\n",
    "labeled_df = dataset.to_pandas()\n",
    "labeled_df['aae'] = df['text']\n",
    "\n",
    "# Save the labeled dataset to a CSV file\n",
    "output_path = './labeled/Phi-3-SAE.csv'\n",
    "labeled_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"SAE dataset saved to {output_path}\")\n",
    "print(f\"Number of processed sentences: {len(labeled_df)}\")\n",
    "print(f\"Number of Unknown sentiments: {(labeled_df['sentiment'] == 'Unknown').sum()}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a74a043-f693-431b-bc5c-b619fa7c2c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to pandas DataFrame\n",
    "labeled_df = dataset.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9916f6ab-7780-4ac2-accd-b3eecb589192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard_american_english    <|user|>\\nFollowing is a tweet extracted from ...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(labeled_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5251004a-e294-421a-8bee-0337ed539ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'standard_american_english': '<|user|>\\nFollowing is a tweet extracted from a African American twitter individual\\'s account. Your task is to convert the tweet to Standard American English. Reply with just the sentence.\\n\"Bitch cant get shit from me but bubble gum nd hard dick from me told da bitch im tryna make a flip im shootin dice wit er rent money !\"<|end|>\\n<|assistant|> \"She can\\'t get anything from me, but bubble gum and a hard work ethic from me told her I\\'m trying to make a change; I\\'m betting on dice with my rent money!\"'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "774d9140-de21-4a4a-a47a-124887e440ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           standard_american_english  \\\n",
      "0  She can't get anything from me, but bubble gum...   \n",
      "1  @islandboi_B Yes, that's what's up. Nothing li...   \n",
      "2  Mixed, huh! Those prominent knees and elbows w...   \n",
      "3  The player Mike James from the Mavs is not imp...   \n",
      "4  It took a whole stranger to tell me he is prou...   \n",
      "\n",
      "                                                 aae  \n",
      "0  Bitch cant get shit from me but bubble gum nd ...  \n",
      "1  @islandboi_B yes that's what's up. Nothin like...  \n",
      "2  Mixed huh !? Those black ass knees and elbows ...  \n",
      "3  The bul Mike James from @mavs ain't shit n he ...  \n",
      "4  It took for a whole stranger to tell me he PRO...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the labeled dataset\n",
    "labeled_df = pd.read_csv('./labeled/Phi-3-SAE.csv')\n",
    "\n",
    "def extract_sae(sentence):\n",
    "    try:\n",
    "        # Extract the part after the prompt and clean it\n",
    "        sae_sentence = sentence.split('<|assistant|>')[-1].strip().strip('\"')\n",
    "        return sae_sentence\n",
    "    except IndexError:\n",
    "        # Handle cases where the split might not work as expected\n",
    "        return sentence.strip().strip('\"')\n",
    "\n",
    "# Apply the extraction function to the 'standard_american_english' column\n",
    "labeled_df['standard_american_english'] = labeled_df['standard_american_english'].apply(extract_sae)\n",
    "\n",
    "# Save the cleaned dataset to a new CSV file\n",
    "output_path = './labeled/Phi-3-SAE-cleaned.csv'\n",
    "labeled_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(labeled_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f728a35b-6949-4bb9-8e37-43316a046db6",
   "metadata": {},
   "source": [
    "## SAE labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cc9b2ca9-5a7b-4a61-be78-e5ed73f5b776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494b5b251ca541f6a1830646f594d350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e872c93fbb4d7eb1c5dca58e45d7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "\"['standard_american_english'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/local/home/furquanh/tmp/ipykernel_1321427/3808687740.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Save the labeled dataset to a CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./labeled/Phi-3-SAE-Labels.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mlabeled_df_sae\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"standard_american_english\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sae_labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"SAE dataset saved to {output_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3897\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3898\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3899\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3901\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6112\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6116\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/home/furquanh/miniconda3/envs/augmentation-project/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6177\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6178\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6180\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['standard_american_english'] not in index\""
     ]
    }
   ],
   "source": [
    "sae = pd.read_csv('./labeled/Phi-3-SAE-cleaned.csv')\n",
    "def create_sae_prompt(example):\n",
    "    return f\"\"\"<|user|>\n",
    "Your task is to analyze the provided sentences written in Standard American English and identify the sentiment expressed by the author. The sentiment should be classified as Positive, Negative, or Neutral for each sentence. Reply with just the sentiment.\\n\n",
    "\"{example['standard_american_english']}\"<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "    \n",
    "# Create a Hugging Face Dataset\n",
    "sae_labelsdataset = Dataset.from_pandas(sae)\n",
    "\n",
    "# Add prompts to the dataset\n",
    "sae_labelsdataset = sae_labelsdataset.map(lambda example: {\"prompt\": create_sae_prompt(example)})\n",
    "\n",
    "# Function to generate sentiments\n",
    "def generate_sae(examples):\n",
    "    outputs = pipe(\n",
    "        examples[\"prompt\"],\n",
    "        max_new_tokens=150,\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    return {\"sae_labels\": [output[0][\"generated_text\"] for output in outputs]}\n",
    "\n",
    "# Generate sentiments\n",
    "sae_labelsdataset = sae_labelsdataset.map(\n",
    "    generate_sae,\n",
    "    batched=True,\n",
    "    batch_size=32,  # Adjust based on your GPU memory\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "# Convert back to pandas DataFrame\n",
    "labeled_df_sae = sae_labelsdataset.to_pandas()\n",
    "\n",
    "\n",
    "# Save the labeled dataset to a CSV file\n",
    "output_path = './labeled/Phi-3-SAE-Labels.csv'\n",
    "labeled_df_sae[[\"standard_american_english\", \"sae_labels\"]].to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"SAE dataset saved to {output_path}\")\n",
    "print(f\"Number of processed sentences: {len(labeled_df)}\")\n",
    "print(f\"Number of Unknown sentiments: {(labeled_df['sentiment'] == 'Unknown').sum()}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e62bacc-897a-42a2-b0d6-1e4efc0c0590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "augment",
   "language": "python",
   "name": "augmentation-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
